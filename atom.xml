<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Sparkera]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://sparkera.ca/"/>
  <updated>2016-01-30T03:00:58.000Z</updated>
  <id>http://sparkera.ca/</id>
  
  <author>
    <name><![CDATA[Sparkera]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Scala Call by Value vs. Name]]></title>
    <link href="http://sparkera.ca/2016/02/01/Scala-Func-Call/"/>
    <id>http://sparkera.ca/2016/02/01/Scala-Func-Call/</id>
    <published>2016-02-01T05:00:00.000Z</published>
    <updated>2016-01-30T03:00:58.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/scala.png" alt=""></p>
<p>From today, I start working on series of articles about how Scala is special and powerful than regular programming languages, such as Java, under tag <a href="http://sparkera.ca/tags/scala/">scalatips</a>. If you have any confused topic, please feel free to contact me so that I can try to use simple terms and explanations to help you understand it.</p>
<p>Usually, there are a couple of ways when calling functions, such as call-by-reference, call-by-value and call-by-name. </p>
<ol>
<li>call-by-reference: we are passing an alias of the variable to a function</li>
<li>call-by-value: we are passing the value of formula/variable to a function. </li>
<li>call-by-name: we are passing the whole formula to a function (there is no such saying when the argument is variable).</li>
</ol>
<p><strong>Note:</strong> For C, C++, they support 1. and 2. For Java, it only supports 2. For Scala, it supports 2, and 3.</p>
<p>Below is an example to demonstrate the difference in Scala.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//<span class="operator"><span class="keyword">Create</span> a printer utility <span class="keyword">for</span> demo. </span><br><span class="line"><span class="keyword">def</span> printer() = &#123;</span><br><span class="line"> println(<span class="string">"calling printer"</span>)</span><br><span class="line"> <span class="number">1</span> // <span class="keyword">return</span> <span class="keyword">value</span></span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">//This <span class="keyword">is</span> a <span class="keyword">call</span>-<span class="keyword">by</span>-<span class="keyword">value</span> syntax <span class="keyword">like</span> regular <span class="keyword">function</span></span><br><span class="line"><span class="keyword">def</span> callByValue(x: <span class="built_in">Int</span>) = &#123;</span><br><span class="line"> println(<span class="string">"x1="</span> + x)</span><br><span class="line"> println(<span class="string">"x2="</span> + x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//This <span class="keyword">is</span> a <span class="keyword">call</span>-<span class="keyword">by</span>-<span class="keyword">value</span> syntax <span class="keyword">by</span> <span class="keyword">using</span> =&gt;</span><br><span class="line"><span class="keyword">def</span> callByName(x: =&gt; <span class="built_in">Int</span>) = &#123;</span><br><span class="line"> println(<span class="string">"x1="</span> + x)</span><br><span class="line"> println(<span class="string">"x2="</span> + x)</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>Then, let’s try the difference by calling them one by one. The result is as follows.<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; callByValue<span class="list">(<span class="keyword">printer</span><span class="list">()</span>)</span></span><br><span class="line">calling printer</span><br><span class="line">x1=1</span><br><span class="line">x2=1</span><br></pre></td></tr></table></figure></p>
<p>For above example, the function firstly evaluate the <code>printer()</code> function. As a result, it prints “calling printer” and value <code>1</code>. Then, the value <code>1</code> is assigned to the argument for function <code>callByValue</code>. Then, the <code>callByValue</code> function prints the x twice for x1 and x2.</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; callByName<span class="list">(<span class="keyword">printer</span><span class="list">()</span>)</span></span><br><span class="line">calling printer</span><br><span class="line">x1=1</span><br><span class="line">calling printer</span><br><span class="line">x2=1</span><br></pre></td></tr></table></figure>
<p>For above example, the function <code>printer</code> will pass into <code>x</code> as whole twice since the <code>x</code> is being called by <code>callByName</code> twice. As a result, the “calling printer” are shown twice.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/scala.png" alt=""></p>
<p>From today, I start working on series of articles about how Scala is special and powerful tha]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Use UDF in Spark DataFrame]]></title>
    <link href="http://sparkera.ca/2016/01/17/Use-UDF-Spark-DF/"/>
    <id>http://sparkera.ca/2016/01/17/Use-UDF-Spark-DF/</id>
    <published>2016-01-17T05:00:00.000Z</published>
    <updated>2016-01-17T18:33:08.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/spark_func.png" alt=""></p>
<p>It is very convenient to create, register, and use user define functions with data. In addition, the recent release of Apache Spark also supports writing user-defined aggregation functions <a href="https://issues.apache.org/jira/browse/SPARK-3947" target="_blank" rel="external">UDAF</a>. Below is a short piece of code to demonstrate creating and using UDF in spark shell. It is quite often that data engineers massage the data and create necessary toolkit functions. The data analyst or business analyst start using them through either Hive over spark, Spark SQL, or other BI tools.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">//<span class="operator"><span class="keyword">Create</span> a <span class="keyword">case</span> <span class="keyword">class</span> struncture <span class="keyword">to</span> hold the <span class="keyword">data</span> <span class="keyword">for</span> demo. </span><br><span class="line">//You can also directly <span class="keyword">convert</span> Hive <span class="keyword">table</span> <span class="keyword">into</span> DF, </span><br><span class="line">//such <span class="keyword">as</span> sqlContext.<span class="keyword">table</span>(<span class="string">"hive_table_name"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">class</span> StockPrice(symbol:<span class="keyword">String</span>, price:Seq[<span class="keyword">Double</span>])</span><br><span class="line"></span><br><span class="line">//<span class="keyword">create</span> the demo <span class="keyword">data</span></span><br><span class="line">val <span class="keyword">data</span>=sc.parallelize(</span><br><span class="line">Seq(</span><br><span class="line">StockPrice(<span class="string">"APPL"</span>, Seq(<span class="number">93.5</span>, <span class="number">95.6</span>, <span class="number">102.7</span>)),</span><br><span class="line">StockPrice(<span class="string">"GOOG"</span>, Seq(<span class="number">604.5</span>, <span class="number">603.7</span>, <span class="number">614.1</span>)),</span><br><span class="line">StockPrice(<span class="string">"BABA"</span>, Seq(<span class="number">64.8</span>, <span class="number">95.2</span>, <span class="number">96.0</span>))</span><br><span class="line">)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Convert</span> the <span class="keyword">data</span> <span class="keyword">to</span> a DataFrame</span><br><span class="line">val df=<span class="keyword">data</span>.toDF(<span class="string">"symbol"</span>, <span class="string">"price"</span>)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Register</span> the results <span class="keyword">to</span> a temp <span class="keyword">table</span></span><br><span class="line">//Alternativelly, you can materilize the <span class="keyword">data</span> <span class="keyword">in</span> Hive <span class="keyword">by</span> </span><br><span class="line">//df.saveAsTable(<span class="string">"stock_price"</span>)</span><br><span class="line">df.registerTempTable(<span class="string">"stock_price"</span>)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Create</span> a <span class="keyword">function</span> that <span class="keyword">get</span><span class="string">'s the average race time</span><br><span class="line">def avgStockPrice(price:Double*) = &#123;</span><br><span class="line">var totalPriec=0.0</span><br><span class="line">price.foreach(x =&gt; totalPriec+=x)</span><br><span class="line">totalPriec/price.size</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//Register that function with the SQLContext'</span>s UDF Registry</span><br><span class="line">//- means <span class="keyword">to</span> <span class="keyword">register</span> <span class="keyword">as</span> partital applied <span class="keyword">function</span></span><br><span class="line">sqlContext.udf.<span class="keyword">register</span>(<span class="string">"avgStockPrice"</span>, avgStockPrice _)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Use</span> the UDF <span class="keyword">in</span> a <span class="keyword">Query</span></span><br><span class="line">sqlContext.<span class="keyword">sql</span>(</span><br><span class="line"><span class="string">"select symbol, avgStockPrice(price) avg_stock_price</span><br><span class="line"> from stock_price"</span>).<span class="keyword">collect</span>().foreach(println)</span></span><br></pre></td></tr></table></figure>
<p>The result is as follows.<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[APPL,<span class="number">97.26666666666667</span>]</span><br><span class="line">[GOOG,<span class="number">607.4333333333334</span>]</span><br><span class="line">[BABA,<span class="number">85.33333333333333</span>]</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/spark_func.png" alt=""></p>
<p>It is very convenient to create, register, and use user define functions with data. In a]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Happy New Year 2016]]></title>
    <link href="http://sparkera.ca/2016/01/01/Happy-New-Year-2016/"/>
    <id>http://sparkera.ca/2016/01/01/Happy-New-Year-2016/</id>
    <published>2016-01-01T05:00:00.000Z</published>
    <updated>2016-01-29T02:03:57.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/happy2016.jpg" alt=""><br>It is the end of 2015 and HAPPY NEW YEAR - 2016. It is time to wrap up my writing calendar with some summary on Sparkera, myself, and Big Data ecosystem.</p>
<p>In past 2015, I have published 21 articles in this blog, which finally got its name as SPARKERA - an era when a little spark can lead big fires. In this year, I also successfully migrated this blog from Jekyll to Hexo and renovate it with new UI and domain name. I also have my first book on Apache Hive published and start my career as an independent consultant for more challenge and exciting moment. In addition, I got more chances to share and teach my experience with many other people through various of courses, meetings, and talks. The year of 2015 is not easy but productive.</p>
<p>In 2015, there are also greats ideas and projects we cannot neglect in the big data ecosystem as follows.</p>
<ul>
<li><a href="http://kylin.apache.org/" target="_blank" rel="external">Apache Kylin</a> - is really an innovative idea by providing SQL interface and multi-dimensional analysis (OLAP) on Hadoop and HBase. This tool has more use cases for enterprise users and it is a better solution when the enterprise wants to build a data warehouse on top of Hadoop ecosystem.</li>
<li><a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafaka</a> - a high-throughput distributed messaging system build for great scalability, high availability, ease of usage, as well as big data friendly.</li>
<li><a href="http://getkudu.io/" target="_blank" rel="external">Apache Kudu</a> - this is a smart solution by providing a super fast columnar storage having access pattern between HDFS and HBase. Kudu can well integrate with the Hadoop Ecosystem. Kudu is suitable for Data warehouse environment and interesting to see how it performs on columnar storage formats like Parquet, and ORC format.</li>
<li><a href="https://nifi.apache.org/" target="_blank" rel="external">Apache NiFi</a> - this is an easy to use, powerful, and reliable dataflow system. It has combined functionality between Apache Flume and ETL tools. HDP has integrated this tool in its latest distribution called Hortonworks Data Flow.</li>
<li>Apache HDFS <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">Cache</a> and <a href="https://issues.apache.org/jira/browse/HDFS-3107" target="_blank" rel="external">Truncate</a> - These two new features are expected for a long time to foresee Hadoop’s future strength as the core component in the big data ecosystem.</li>
<li>Apache YARN - new features supporting management label, long running jobs, and dock support</li>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="external">Spark SQL and DataFrame</a> - Spark SQL brings Spark more closed to enterprise use case by leveraging the dataframe having schema and optimization on regular RDD. According to the benchmark, dataframe have great performance boost than regular RDD.</li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started" target="_blank" rel="external">Hive Over Spark</a> - makes Hive users/application to leverage powerful in-memory computing engine of Apache Spark.</li>
<li><a href="http://geode.incubator.apache.org/" target="_blank" rel="external">GemFire</a> and <a href="http://hawq.incubator.apache.org/" target="_blank" rel="external">HAWQ</a> - Very few enterprise would like to open source their leading products, but Pivotal did it and did for all its big data products. This is a great time to see the gap between enterprise ready products and open source softwares. This will also bring challenge and competitions for solutions, such as Impala, Hive, Spark SQL, HBase.</li>
<li><a href="https://zeppelin.incubator.apache.org/" target="_blank" rel="external">Apache Zeppelin</a> - There is very few open source visualization tools avaliable, especially which supporting many framework or system. Its native supports Apache Spark and interactive code-to-result experience attracting lots of community users.</li>
</ul>
<p>Well, again at the end, look forward a mazing and beautiful year of 2016 for me and you!</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/happy2016.jpg" alt=""><br>It is the end of 2015 and HAPPY NEW YEAR - 2016. It is time to wrap up my writing calendar wi]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Build Big Data Warehouse With Apache Hive]]></title>
    <link href="http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive%20copy/"/>
    <id>http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive copy/</id>
    <published>2015-12-21T01:50:58.000Z</published>
    <updated>2015-12-31T02:49:48.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-04apache-hive" title="Ten tools for ten big data areas 04_Apache Hive" target="_blank">Ten tools for ten big data areas 04_Apache Hive</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the fourth topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://hive.apache.org" target="_blank" rel="external">Apache Hive</a> is one of the earliest SQL on Hadoop approach. Although its legacy design is based on MapReduce, Hive involves fast and tries to be shining continually by providing sub-seconds query on top of Hadoop in future roadmap.</p>
<p>There are following areas where Hive could consider involving in the future.</p>
<ol>
<li><p>Dynamic Schema - Hive’s meta store provides a convenient view of schemaless data in the traditional schema view of data in RDBMS. This is a good approach for the usage transaction from the legacy database. But, there are increasing requirements for dynamically creating the schema which means we do not have to define the schema to access the data, especially for semi-structure and structure data. This is very useful when doing ad-hoc analysis. And, there are already other tools working in this way, such as pig, spark dataframe, apache drill, etc.</p>
</li>
<li><p>Smart Engine - There are two subareas where we define engine as smart, various and transparent.  For various, Hive has already supported working with different computing engines, such as MapReduce, Tez, and Spark. For transparent, we expect to see the switch between the different engine in a dynamic or even transparent way. There are always pros and cons for using different engines. If we can dynamic specify which engine to use by adding specific SQL keywords/hints, it will be an awesome feature for Apache Hive. As a result, we can use a single framework (hive) for the different use cases. Even smarter, the framework can pick up the best engine to run queries on the fly.</p>
</li>
<li><p>Multi-source Support. This is where we expect Hive can be a unified SQL over various of data sources so that we can easily to do data blending among different type of data sources.</p>
</li>
<li><p>Others -  Standard SQL-2011 support, store procedure like UDF, Live Long and Process (LLAP), leverage Hadoop <a href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">caching</a> features, metastore performance improvement, and advanced transaction supports. </p>
</li>
</ol>
<p><img src="/images/apachehive.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Light Big Data With Apache Spark]]></title>
    <link href="http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/"/>
    <id>http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/</id>
    <published>2015-12-20T02:10:58.000Z</published>
    <updated>2015-12-23T22:20:18.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-03apache-spark" title="Ten tools for ten big data areas 03_Apache Spark" target="_blank">Ten tools for ten big data areas 03_Apache Spark</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the third topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> is one of the greatest big data open source projects in nowadays. </p>
<p>There are so many articles, reports about how great the Apache Spark it is. I also heard lots of people keep saying that Spark will replace Hadoop and do everything in big data. As for me, I agree on some of them by saying Spark does a great job on some areas. However, I do disagree some saying about spark and I’ll talk about this today as most people do not realise there Spark may not work very well in some areas. </p>
<p>First, Spark is not going to replace Hadoop. Hadoop is a big data platform while Spark is an application. For another saying in terms of IPhone ecosystem, it is a relationship between IOS and Apps. Hadoop, especially Yarn, gradually become more and more important in the role of a platform by providing a multi-purpose universal platform for run various of big data applications. On the other hand, Spark is a powerful big data application which is able to do lots of things in big data. But it cannot “rules all” the big data ecosystem. We still have lots of use cases which require other big data applications. </p>
<p>Second, Spark is not good for everything. Below are some areas that I think we have other better options than using Spark.</p>
<ul>
<li><p>Spark has to rely on HDFS or other file systems to store data. It is a mainly computing engine. Spark is based on RDD, which is the immutable dataset. As a result, Spark does not fit for the use case where you need to modify the data. </p>
</li>
<li><p>Spark uses lots of micro-batch execution model to simulate data streaming. As a result, it has a limitation when the stream interval less than 0.5 seconds. For instead, you may need other truly real-time streaming framework, such as Apache <a href="http://storm.apache.org" target="_blank" rel="external">Storm</a> or <a href="http://flink.apache.org" target="_blank" rel="external">Flink</a>. </p>
</li>
<li><p>Spark runs on the JVM and leverages Java’s garbage collector. As JVM is designed for its general purpose, it lacks flexibility, good user experience, as well as efficient memory usage. Spark team has realised about this and comes up the Project Tungsten, which starts to build Spark’s own memory management system in the recent release. </p>
</li>
<li><p>For data ETL (extract, transformation, and load), you may not always need Spark’s speed, but focus more on the reliability as well as failure recovery. In this case, MapReduce’s processing style can be just fine as stable batch-mode processing. </p>
</li>
<li><p>Spark aims to replace MapReduce, but it does not provide any way to back compatible with old MapReduce jobs. However, there are legacy MapReduce jobs which could not be retired immediately from production.</p>
</li>
<li><p>MLLib in Spark still needs improvement by supporting more algorithm as well as accuracy.</p>
</li>
<li><p>GrapX in Spark is still new (comparing <a href="http://giraph.apache.org/" target="_blank" rel="external">Apache Giraph</a>) and some functions are only available in Scala API.</p>
</li>
</ul>
<p>Although there are areas to improve for Apache Spark, there is no doubt Apache Spark is a great big data application stack. I do believe it has good future. As there are big competition and fast evolvement in the big data ecosystem, let’s look forward to seeing if this little ‘spark’ can start a prairie fire.<br><img src="/images/apachespark.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Spark Memo]]></title>
    <link href="http://sparkera.ca/2015/12/12/Apache-Spark-Memo/"/>
    <id>http://sparkera.ca/2015/12/12/Apache-Spark-Memo/</id>
    <published>2015-12-12T14:04:58.000Z</published>
    <updated>2015-12-12T02:39:36.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benefit fot this collection.</p>
<h3 id="1-_Spark_Core">1. Spark Core</h3><ol>
<li><p>The stage creation rule is based on the idea to pipeline as many narrow transformations as possible. Once stages are figured out, spark will generate tasks from stages. The first stage will create ShuffleMapTasks and the last stage will create ResultTasks because in the last stage, one action operation is included to produce results.</p>
</li>
<li><p>The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks : one task per partition. Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition.</p>
</li>
<li><p>At high level, there are two transformations that can be applied onto the RDDs, namely narrow transformation and wide transformation. Wide transformations basically result in stage boundaries. <strong>Narrow transformation</strong> - doesn’t require the data to be shuffled across the partitions. for example, Map, filter and etc.. <strong>Wide transformation</strong> - requires the data to be shuffled for example, reduceByKey and etc..</p>
</li>
<li><p><strong>Partition</strong> - all the data you work with in Spark is split into partitions. What a single partition is and how is it determined? Partition size completely depends on the data source you use. For most of the methods to read the data in Spark you can specify the amount of partitions you want to have in your RDD. When you read a file from HDFS, you use Hadoop’s InputFormat to make it. By default each input split returned by InputFormat is mapped to a single partition in RDD. For most of the files on HDFS single input split is generated for a single block of data stored on HDFS, which equals to approximately 64MB of 128MB of data. Approximately, because the data in HDFS is split on exact block boundaries in bytes, but when it is processed it is split on the record splits. For text file the splitting character is the newline char, for sequence file it is the block end and so on. The only exception of this rule is compressed files – if you have the whole text file compressed, then it cannot be split into records and the whole file would become a single input split and thus a single partition in Spark and you have to manually repartition it.</p>
</li>
<li><p>Spark’s basic abstraction is the Resilient Distributed Dataset, or RDD. The RDD is how Spark simplifies complex operations like join or groupBy and hides the fact that under the hood, you’re dealing with fragmented data. That fragmentation is what enables Spark to execute in parallel, and the level of fragmentation is a function of the number of partitions of your RDD. The number of partitions is important because a stage in Spark will operate on one partition at a time (and load the data in that partition into memory). Consequently, if you have fewer partitions than active stages, you will wind up under-utilizing your cluster. Furthermore, since with fewer partitions there’s more data in each partition, you increase the memory pressure on your program. On the flip side, with too many partitions, your performance may degrade as you take a greater hit from network and disk I/O. Ultimately this concept ties into Spark’s notion of parallelism and how you can tune it (see the discussion of tuning parallelism here) to optimize performance.</p>
</li>
<li><p>The property <strong>spark.cleaner.ttl</strong> parameter to trigger automatic cleanups.</p>
</li>
<li><p>Keep in mind that repartitioning your data is a fairly expensive operation. Spark also has an optimized version of repartition() called coalesce() that allows minimum the data movement, but only if you are decreasing the number of RDD partitions.</p>
</li>
<li><p>In a typical Spark node，40% of the memory is used for computing while 60% is used for storing the data.</p>
</li>
<li><p><strong>cache()/persist()</strong> is also lays operation. However, unpersist() is non-lazy operation.</p>
</li>
</ol>
<h3 id="2-_Spark_SQL">2. Spark SQL</h3><ol>
<li><p>When working with a HiveContext, DataFrames can also be saved as persistent tables using the saveAsTable command. Unlike the registerTempTable command, saveAsTable will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SQLContext with the name of the table.</p>
</li>
<li><p>By default saveAsTable will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped. However, Spark can also create temp table from data frame using rdd.registerTempTable(“table_name”). These tables are out of control by hive.</p>
</li>
<li><p>Spark SQL also supports reading and writing data stored in Apache Hive. However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the -Phive and -Phive-thriftserver flags to Spark’s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benef]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Tableau Your Big Data]]></title>
    <link href="http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/"/>
    <id>http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/</id>
    <published>2015-12-12T02:04:58.000Z</published>
    <updated>2015-12-19T01:06:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-02tableau" title="Ten tools for ten big data areas 02_Tableau" target="_blank">Ten tools for ten big data areas 02_Tableau</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the second topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://www.tableau.com" target="_blank" rel="external">Tableau</a> is one of few commercial soft that I have to recommend in the visualization area for big data.</p>
<p>The term visualization becomes very popular when tableau software comes into the picture. As I remember, It was four or five years ago. When we first time to see how business intelligence can be like this and how data visualization looks like. Tableau software comes up the concept of self BI or BI without developers. This really scares me as I am a developer. But later, I have said it is correct. With the help of tableau, people can more focus on the meaning of data instead the old ways we deal with reporting. In addition, it is amazing fast speed and user experience makes it top notch in the data visualization domain and business intelligence. In recent years, we can see lots of traditional BI company start doing what was tableau doing, but it is clear it is far more away in terms of performance and sense of experience.</p>
<p><img src="/images/tableauyourdata.jpg" alt=""><br>Tableau is very first few of tools which provide connectors for big data from Hadoop Hive connector, impala, to Spark SQL. Tableau also announced new direct-connection capabilities with InfoSphere BigInsights from IBM, along with new beta connectors for Amazon Elastic MapReduce from Amazon Web Services Inc.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="tableau" scheme="http://sparkera.ca/tags/tableau/"/>
    
      <category term="visualization" scheme="http://sparkera.ca/tags/visualization/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ten Tools for Ten Big Data Areas]]></title>
    <link href="http://sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/"/>
    <id>http://sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/</id>
    <published>2015-11-28T21:04:58.000Z</published>
    <updated>2015-12-21T01:25:25.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special magic and power. Anyone who can own one of these weapons could become a master or leader who is not undefeatable.<br><img src="/images/10weapons.png" alt=""></p>
<p>Nowadays, the big data ecosystem becomes bigger and bigger. There are thousands of players in the big data landscape actively paying right now by creating thousands of tools, framework, solution, etc. However, I believe the legend are always among few of them. I choose 10 of the greatest tools as my recommendation for people really want to learn or use big data (see headline picture). </p>
<p>I have started the journey to introduce each of them which are free or not, but I guarantee the best from my experience. Here is the tag <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a> for this series of presentations as well as direct link for each of them. Let’s look forward all of them in future…</p>
<ol>
<li><a href="http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/">Informatica in Big Data</a></li>
<li><a href="http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/">Tableau Your Big Daya</a></li>
<li><a href="http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/">Light Big Data with Apache Spark</a></li>
<li><a href="http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive/">Build Big Data Warehouse with Apache Hive</a></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special ]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Informatica in Big Data]]></title>
    <link href="http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/"/>
    <id>http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/</id>
    <published>2015-10-25T03:04:58.000Z</published>
    <updated>2015-12-19T01:09:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-01-informatica-54334595" title="Ten tools for ten big data areas 01 informatica " target="_blank">Ten tools for ten big data areas 01 informatica </a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the first topic I’ll cover for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. I came up this series of topics from ten artifacts from ancient China when people are honored to get one of the ten artifacts.</p>
<p>In terms of data integrations of big data, I pick up Informatica, which is used to be a public company in NASDAQ as INFA. In the middle of this year, Informatica announced the successful completion of its acquisition by a company controlled by the Permira funds and Canada Pension Plan Investment Board (CPPIB). Additionally Informatica announced that Microsoft Corporation and Salesforce Ventures have agreed to become strategic investors in the company alongside the Permira funds and CPPIB. The acquisition is valued at approximately $5.3 billion, with Informatica stockholders receiving $48.75 in cash per share.</p>
<p>Especially in big data area, Informatica has leading product called Informatica big data edition - developer. This is brand new tool in the Informatica family. It has new user interface based on Eclipse. It is single tool including development all ETL job components. In Informatica developer, this is no more session. For instead, there is no concept called application. We can add mapping or workflow in the application to deploy and run the bulk of ETL jobs as an application.</p>
<p>The main advantage of Informatica developer is that it converts the ETL logic/mapping into Hive query and execute it on top of Hadoop cluster. For example, you can even push a none-Hadoop related jobs running on top of Hadoop. This advantage not only make ETL job leverages the power of computing resource of Hadoop but also get rid of additional budget for a dedicated ETL server cluster like what’s in the Informatica PowerCenter period. In addition, this design has lots of potential when Hive evolves in the big data ecosystem, such as Hive over Spark. In future, Informatica developer will be able to leverage more distributed computing framework beyond of Hadoop, such as Spark for better performance.</p>
<p>However, there are still limitations for this tool which is still new in the Informatica family. Below are some limitations I come across recently.</p>
<ul>
<li>Does not support Hive table in complex formats, such as Avro, etc</li>
<li>Does not support write into buckets tables</li>
<li>Does not support using parameters in row level, complex data objects path.</li>
<li>Does not support to return target successful or failed rows from mapping</li>
<li>Cannot run the workflow or application straightforward in the developer tool</li>
<li>None of errors and exceptions are reported at run time when running in Hive mode</li>
<li>Overall reliability need to be improved, such as OOM, exception on data adapters</li>
</ul>
<p>Alternatives, there are also other ETL tools for choice. <a href="https://www.talend.com/" target="_blank" rel="external">Talend</a> and <a href="http://www.pentaho.com/" target="_blank" rel="external">Pentaho</a> all provide big data version of tools. But, their big data version almost the same to the regular one except having common big data tool connector shipped. Therefore, these tools can read or write data between Hadoop rather than running on top of Hadoop.</p>
<p>There is another new data flow tool which people may pay attention, <a href="https://nifi.apache.org/" target="_blank" rel="external">Apache NiFi</a>, which supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. The company behind this tool is called Onyara, an early-stage startup acquired by Hontonworks in Auguest of 2015.<br><img src="/images/bde.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="informatica" scheme="http://sparkera.ca/tags/informatica/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cloudera Launches One Platform Initatives To Advance Spark]]></title>
    <link href="http://sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/"/>
    <id>http://sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/</id>
    <published>2015-09-23T15:27:57.000Z</published>
    <updated>2015-09-24T02:56:23.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.</p>
<p>The Spark is originally invented by few guys who started up the <a href="https://databricks.com/" target="_blank" rel="external">Databrick</a>. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn. As a result, more and more companies start switching their MapReduce jobs to Spark and few of them already have big cluster deployed in production. Few months earlier, IBM has claimed that they would have 5000+ developers working in Apache Spark to make it better (It is heard that the core DB2 development force are reassigned to this new mission inside the IBM, not sure.). Clearly, the One Platform initiative is an echo for IBM’s saying from Cloudera who always believe itself a leader in the domain in big data. Cloudera is not likely to leave IBM alone to take this delicious fruit - Spark. We are waiting for more actions from other company, such as Hortonworks, MapR. I do not believe they just keep silent. Or maybe there is the underlying discussion to acquire the Databrick, who knows.</p>
<p>The One Platform initiative has covers four areas of efforts including security, scale, management, streaming. For more information regarding the One Platform initiative, please refer to the <a href="http://vision.cloudera.com/one-platform/" target="_blank" rel="external">Cloudera post</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that]]>
    </summary>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Get Git Modified But Untracked Content Checked In]]></title>
    <link href="http://sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/"/>
    <id>http://sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/</id>
    <published>2015-08-21T15:27:57.000Z</published>
    <updated>2015-08-22T18:11:54.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the <code>git add .</code> and <code>git status</code>. It shows below error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit. I only realize that I miss the theme files when I try to rebuild the Hexo site from home.<br><img src="/images/gituntracked.png" alt=""><br>After searching a while form Google, I got my issues resolved and share the steps below for reference.</p>
<ul>
<li>Removed the .git directories from the directories (In my case, ../theme/hueman/.git)</li>
<li>Ran git rm -rf –cached <the untracked="" directory=""> (In my case, /C/Users/ddu/Git/sparkera/themes/hueman)</the></li>
<li>Re-added the directories with a <code>git add .</code> and check by <code>git status</code>.</li>
</ul>
<p>Then all the untracked files are added. Then you can do <code>git commit -m</code> and <code>git push</code> the submit all the changes.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo p]]>
    </summary>
    
      <category term="git" scheme="http://sparkera.ca/tags/git/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop Streaming]]></title>
    <link href="http://sparkera.ca/2015/06/21/Hadoop%20Streaming/"/>
    <id>http://sparkera.ca/2015/06/21/Hadoop Streaming/</id>
    <published>2015-06-21T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic API which allows writing Mappers and Reduces in any language. </p>
<ul>
<li>Develop MapReduce jobs in practically any language</li>
<li>Uses Unix Streams as communication mechanism between Hadoop and your code</li>
<li>Any language that can read standard input and write are supported</li>
</ul>
<p>Few good use-cases:</p>
<ul>
<li>Text processing - scripting languages do well in text analysis</li>
<li>Utilities and/or expertise in languages other than Java</li>
</ul>
<h4 id="2-_Process_Flow">2. Process Flow</h4><p>Below is how streaming processing</p>
<ul>
<li>Map input passed over standard input</li>
<li>Map processes input line-by-line</li>
<li>Map writes output to standard output - Key-value separate by tab</li>
<li>Reduce input passed over standard input<ul>
<li>Same as mapper output – key-value pairs separated by tab</li>
<li>Input is sorted by key</li>
</ul>
</li>
<li>Reduce writes output to standard output</li>
</ul>
<p><img src="/images/hadoopstreaming.png" alt="&quot;avatar&quot;"></p>
<h4 id="3-_Example_of_mapper">3. Example of mapper</h4><p><strong>mapper.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python </span></span><br><span class="line">import sys </span><br><span class="line"><span class="comment"># mapper.py </span></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input) </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>: </span><br><span class="line"><span class="comment"># remove leading and trailing white space </span></span><br><span class="line"><span class="built_in">line</span> = <span class="built_in">line</span>.strip() </span><br><span class="line"><span class="comment"># split the line into words </span></span><br><span class="line"><span class="keyword">words</span> = <span class="built_in">line</span>.<span class="built_in">split</span>() </span><br><span class="line"><span class="comment"># increase counters for word in words: </span></span><br><span class="line"><span class="comment"># write the results to STDOUT (standard output); </span></span><br><span class="line"><span class="comment"># what we output here will be the input for the </span></span><br><span class="line"><span class="comment"># Reduce step, i.e. the input for reducer.py </span></span><br><span class="line"><span class="comment"># tab-delimited; the trivial word count is 1 </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">word</span> <span class="operator">in</span> <span class="keyword">words</span></span><br><span class="line">print <span class="string">'%s\t%s'</span> % (<span class="built_in">word</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="4-_Example_of_reducer">4. Example of reducer</h4><p><strong>reducer.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#reducer.py</span></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">current_word = None</span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line"><span class="built_in">word</span> = None</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>:</span><br><span class="line">    <span class="comment"># remove leading and trailing white space</span></span><br><span class="line">    <span class="built_in">line</span> = <span class="built_in">line</span>.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    <span class="built_in">word</span>, count = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">'\t'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = int(count)</span><br><span class="line">    except ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently ignore/discard this line</span></span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch works because Hadoop sorts map output by key before passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = <span class="built_in">word</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">    print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure>
<h4 id="5-_Run_the_job">5. Run the job</h4><ul>
<li>Test in local mode from Linux pipe</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat testText.txt | mapper.py | sort | reducer.py</span><br><span class="line">a <span class="number">1</span></span><br><span class="line">h <span class="number">1</span></span><br><span class="line">i <span class="number">4</span></span><br><span class="line">s <span class="number">1</span></span><br><span class="line">t <span class="number">5</span></span><br><span class="line">v <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Run in the cluster</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop/yarn jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-*<span class="class">.jar</span> \</span><br><span class="line">-D mapred<span class="class">.job</span><span class="class">.name</span>=<span class="string">"Count Job via Streaming"</span> \</span><br><span class="line">-files <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/mapper<span class="class">.py</span>, <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/reducer<span class="class">.py</span> \</span><br><span class="line">-<span class="tag">input</span> /training/input/hamlet<span class="class">.txt</span> \</span><br><span class="line">-output /training/output/ \</span><br><span class="line">-mapper mapper<span class="class">.py</span> \</span><br><span class="line">-combiner reducer<span class="class">.py</span> \</span><br><span class="line">-reducer reducer.py</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic]]>
    </summary>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Constructor vs. Java Constructor]]></title>
    <link href="http://sparkera.ca/2015/04/20/Scala%20and%20Java%20Constructors/"/>
    <id>http://sparkera.ca/2015/04/20/Scala and Java Constructors/</id>
    <published>2015-04-20T04:00:00.000Z</published>
    <updated>2016-01-30T02:14:53.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="2-_Constructor_With_Private_Attribute">2. Constructor With Private Attribute</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> Bar bar;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="keyword">private</span> <span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="3-_Call_Super_Constructor">3. Call <em>Super</em> Constructor</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() <span class="keyword">extends</span> <span class="title">SuperFoo</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;   </span><br><span class="line">      <span class="keyword">super</span>(bar);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="title">bar</span>:<span class="title">Bar</span>) <span class="keyword">extends</span> <span class="title">SuperFoo</span>(<span class="title">bar</span>) </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-_Multiple_Constructors">4. Multiple Constructors</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span> &#123;  </span><br><span class="line">    <span class="keyword">public</span> Bar bar;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params"></span>) </span>&#123;   </span><br><span class="line">       <span class="keyword">this</span>(<span class="keyword">new</span> Bar());   </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params">Bar bar</span>) </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>. bar = bar;   </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(</span><span class="function"><span class="keyword">val</span> <span class="title">bar</span>:</span><span class="type">Bar</span>)&#123;  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>) = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">Bar</span>)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-_Methods_of_getter_and_setter">5. Methods of <em>getter</em> and <em>setter</em></h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125; </span><br><span class="line">   <span class="function"><span class="keyword">public</span> Bar <span class="title">getBar</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">return</span> bar;   </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBar</span><span class="params">(Bar bar)</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>1. Scala Code</strong></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="import"><span class="keyword">import</span> scala.reflect._  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Foo</span><span class="container">(@<span class="type">BeanProperty</span> <span class="title">var</span> <span class="title">bar</span>:<span class="type">Bar</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>2. Scala Code</strong></p>
<pre><code><span class="keyword">import</span> scala.reflect._  
<span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(aBar:Bar) {  
    @BeanProperty  
    <span class="keyword">private</span> <span class="variable"><span class="keyword">var</span> bar</span> = aBar  
}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Ja]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Hive Essentials Published]]></title>
    <link href="http://sparkera.ca/2015/03/15/Apache%20Hive%20Essentials%20Published/"/>
    <id>http://sparkera.ca/2015/03/15/Apache Hive Essentials Published/</id>
    <published>2015-03-15T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<li>My very first book</li>
<li>Also the first book on Apache Hive 1.0.0 in the world</li>
</ul>
<p>Check it out <a href="http://bit.ly/1LRkd5m" target="_blank" rel="external">here</a></p>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/apache-hive-essentials" target="_blank"><img src="/images/hivebooks.jpg" width="150" height="200" align="left"></a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<]]>
    </summary>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive and Hadoop Exceptions]]></title>
    <link href="http://sparkera.ca/2015/02/12/Hive%20and%20Hadoop%20Exceptions/"/>
    <id>http://sparkera.ca/2015/02/12/Hive and Hadoop Exceptions/</id>
    <published>2015-02-12T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/exceptions.png" alt=""></p>
<p>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports following exceptions</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.metadata</span><span class="class">.HiveException</span>:java<span class="class">.io</span><span class="class">.IOException</span>:Filesystem closed</span><br></pre></td></tr></table></figure>
<p>According to the search <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/201207.mbox/%3CCAL=yAAE1mM-JRb=eJGkAtxWQ7AJ3e7WJCT9BhgWq7XDTNxrwfw@mail.gmail.com%3E" target="_blank" rel="external">here</a>, the mainly reason for this is that when multiple nodes read HFDS files if one node is offline, it will throw such exception when the other nodes are still reading the data cached. There are two ways to resolve this.</p>
<ul>
<li>Turn off JVM reuse</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.reuse.jvm.num.tasks<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Disable caches</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.hdfs.impl.disable.cache<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/exceptions.png" alt=""></p>
<p>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports fol]]>
    </summary>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Data Analysis]]></title>
    <link href="http://sparkera.ca/2015/02/06/Data-Analysis/"/>
    <id>http://sparkera.ca/2015/02/06/Data-Analysis/</id>
    <published>2015-02-06T05:00:00.000Z</published>
    <updated>2015-09-23T23:45:21.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/Data-Analysis.jpg" alt=""></p>
<p>A friend of mine asked me what is data analysis. This is a simple but difficult question. It is simple because we talk about data analysis all the time and everywhere. It is difficult because there are so many ways of explaining it at different time. In the ear of big data, I think data analysis have three following areas.</p>
<ul>
<li><p>Flatten Analysis: Analysis is performed on the static data set from single dimentional view. Most analysis are simple enough focusing on the fixed scope of data. It is more or less like statistics, such as total, average, standard devidations, etc. Simple SQL or Excel tools can be used to do flatten data analysis.</p>
</li>
<li><p>Dimentional Analysis</p>
</li>
<li><p>Iterational Analysis</p>
</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/Data-Analysis.jpg" alt=""></p>
<p>A friend of mine asked me what is data analysis. This is a simple but difficult quest]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Data Lake Stages]]></title>
    <link href="http://sparkera.ca/2015/02/02/Data-Lake-Stages/"/>
    <id>http://sparkera.ca/2015/02/02/Data-Lake-Stages/</id>
    <published>2015-02-02T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/datalake.png" align="left"> <br><br><br>Edd has post a very impressive <a href="http://www.forbes.com/sites/edddumbill/2014/01/14/the-data-lake-dream/" target="_blank" rel="external">blog</a> about how Hadoop ecosystem influence the data lake in enterprise recently. It discussed about the four following stages when enterprise’s data evolution  to the dream of data lake. I also share some of mine as addition.</p>
<h5 id="Stage_1_-_Life_Before_Hadoop">Stage 1 - Life Before Hadoop</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage1.png" alt="Life Before Hadoop" title="Life Before Hadoop"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Applications stand alone with their databases</li>
<li>Some applications contribute data to a data warehouse</li>
<li>Analysts run reporting and analytics in data warehouse</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical enterprise data warehouse (EDW) happens. Most of data sets are structured and well-organized. </li>
</ul>
<h5 id="Stage_2_-_Hadoop_Is_Introduced">Stage 2 - Hadoop Is Introduced</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage2.png" alt="Hadoop Is Introduced" title="Hadoop Is Introduced"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Applications contribute data to Hadoop</li>
<li>Hadoop runs batch MapReduce jobs</li>
<li>Hadoop used for ETL into warehouse or analytic databases</li>
<li>Hadoop data reintroduced into applications</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical EDW and hadoop merge happens. Most of data sets are semi-structured and unstructured with structured data. </li>
<li>The hot data is more likely injected to EDW since EDW has better support to BI tools and faster response for ad-hoc query.</li>
<li>Data in Hadoop becomes a center data depository considering the data volume and management cost. Therefore, the EDW also injects its data to the Hadoop. Here is where I disagree with the Edd since I believe it is more like a data exchange (bidirections) instead of single direction in the picture.</li>
<li>Analytics over Hadoop/Big data starts as data verification, long period statistics, and trending calculations.</li>
</ul>
<h5 id="Stage_3_-_Growing_The_Data_Lake">Stage 3 - Growing The Data Lake</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage3.png" alt="Growing The Data Lake" title="Growing The Data Lake"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Newly built systems center around Hadoop by default</li>
<li>Applications use each other’s data via Hadoop</li>
<li>Interactive use of Hadoop as in-Hadoop databases deployed (e.g. Impala, Greenplum, Spark)</li>
<li>Hadoop becomes a default data destination, governance and metadata become important</li>
<li>Data warehouse use becomes the exception, where legacy or special requirements dictate</li>
<li>External data sources integrated via Hadoop</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical EDW being replaced happens. </li>
<li>Analytics over Hadoop becomes well accepted in terms of performance and compatibility.</li>
<li>However, the Hadoop is still playing role of OLAP instead of OLTP</li>
</ul>
<h5 id="Stage_4_-_Data_Lake_And_Application_Cloud">Stage 4 - Data Lake And Application Cloud</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage4.png" alt="Data Lake And Application Cloud" title="Data Lake And Application Cloud"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>New applications are built on a Hadoop application platform around the data lake</li>
<li>Hadoop matures as an elastic distributed data computing platform, for both operational and analytical functions</li>
<li>Data lake adds security and governance layers</li>
<li>Data availability increases, application deployment time decreases</li>
<li>Some apps still have special or legacy needs and execute independently</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical Data Oriented Architecture (DOA) starts.</li>
<li>Hadoop becomes the central, operational, analytical of enterprise data lake.</li>
<li>Different data lakes can also flow/exchange with each other by data services in terms of <strong><em>DATA OCEAN</em></strong></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/datalake.png" align="left"> <br><br><br>Edd has post a very impressive <a href="http://www.forbes.com/sites/edddumbill/]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Setup Spark in MAC]]></title>
    <link href="http://sparkera.ca/2015/01/23/Setup-Spark-In-MAC/"/>
    <id>http://sparkera.ca/2015/01/23/Setup-Spark-In-MAC/</id>
    <published>2015-01-23T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkmac.jpg" alt=""><br>It is great to see that Brew supports install Spark. It makes installation of Spark quite easier in Mac. I just follow few steps to get my spark instance installed locally.</p>
<h4 id="1-_Install_brew_utility-">1. Install brew utility.</h4><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">mymac:</span>$ ruby -e <span class="string">"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</span></span><br><span class="line">==&gt; This script will <span class="string">install:</span></span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>brew</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/Library/</span>...</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/share/</span>man<span class="regexp">/man1/</span>brew<span class="number">.1</span></span><br><span class="line">==&gt; The following directories will be made group <span class="string">writable:</span></span><br><span class="line"><span class="regexp">/usr/</span>local/.</span><br><span class="line"><span class="regexp">/usr/</span>local/bin</span><br><span class="line">==&gt; The following directories will have their group set to <span class="string">admin:</span></span><br><span class="line"><span class="regexp">/usr/</span>local/.</span><br><span class="line"><span class="regexp">/usr/</span>local/bin</span><br><span class="line"></span><br><span class="line">Press RETURN to <span class="keyword">continue</span> or any other key to abort</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/chmod g+rwx /</span>usr<span class="regexp">/local/</span>. <span class="regexp">/usr/</span>local/bin</span><br><span class="line"><span class="string">Password:</span></span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>usr<span class="regexp">/bin/</span>chgrp admin <span class="regexp">/usr/</span>local<span class="regexp">/. /</span>usr<span class="regexp">/local/</span>bin</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/mkdir /</span>Library<span class="regexp">/Caches/</span>Homebrew</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/chmod g+rwx /</span>Library<span class="regexp">/Caches/</span>Homebrew</span><br><span class="line">==&gt; Downloading and installing Homebrew...</span><br><span class="line"><span class="string">remote:</span> Counting <span class="string">objects:</span> <span class="number">226972</span>, done.</span><br><span class="line"><span class="string">remote:</span> Compressing <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">59619</span>/<span class="number">59619</span>), done.</span><br><span class="line"><span class="string">remote:</span> Total <span class="number">226972</span> (delta <span class="number">166103</span>), reused <span class="number">226972</span> (delta <span class="number">166103</span>)</span><br><span class="line">Receiving <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">226972</span><span class="regexp">/226972), 52.13 MiB | 1021.00 KiB/</span>s, done.</span><br><span class="line">Resolving <span class="string">deltas:</span> <span class="number">100</span>% (<span class="number">166103</span>/<span class="number">166103</span>), done.</span><br><span class="line">From <span class="string">https:</span><span class="comment">//github.com/Homebrew/homebrew</span></span><br><span class="line"> * [<span class="keyword">new</span> branch]      master     -&gt; origin/master</span><br><span class="line">HEAD is now at e58a69c <span class="string">points2grid:</span> update <span class="number">1.3</span><span class="number">.0</span> bottle.</span><br><span class="line">==&gt; Installation successful!</span><br><span class="line">==&gt; Next steps</span><br><span class="line">Run `brew doctor` before you install anything</span><br><span class="line">Run `brew help` to get started</span><br></pre></td></tr></table></figure>
<h4 id="2-_Install_the_spark_from_Brew">2. Install the spark from Brew</h4><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mymac</span>:$ brew install apache-spark</span><br><span class="line">=<span class="function">=&gt;</span> Downloading </span><br><span class="line"><span class="attribute">http</span>:<span class="regexp">//</span>d3kbcqa49mib13.cloudfront.net/spark-<span class="number">1.2</span><span class="number">.0</span>-bin-hadoop2<span class="number">.4</span>.t</span><br><span class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span> <span class="number">100.0</span>% /usr/local/Cellar/apache-spark/<span class="number">1.2</span><span class="number">.0</span>: <span class="number">283</span> files, <span class="number">234</span>M, built <span class="keyword">in</span> <span class="number">24.8</span> minutes</span><br></pre></td></tr></table></figure>
<h4 id="3-_Start_the_spark_shell-">3. Start the spark shell.</h4><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">bash-<span class="number">3.2</span>$ spark-shell</span><br><span class="line">Spark <span class="keyword">assembly</span> <span class="keyword">has</span> been built <span class="keyword">with</span> Hive, including Datanucleus jars <span class="keyword">on</span> classpath</span><br><span class="line"><span class="keyword">Using</span> Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: Changing view acls to: dayongd</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: Changing modify acls to: dayongd</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(dayongd); users with modify permissions: Set(dayongd)</span><br><span class="line">15/01/23 20:21:39 INFO HttpServer: Starting HTTP Server</span><br><span class="line">15/01/23 20:21:39 INFO Utils: Successfully started service '</span>HTTP <span class="keyword">class</span> server<span class="string">' on port 56839.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version <span class="number">1.2</span>.<span class="number">0</span></span><br><span class="line">      /_/</span><br><span class="line"> </span><br><span class="line"><span class="keyword">Using</span> Scala version <span class="number">2.10</span>.<span class="number">4</span> (Java HotSpot(TM) <span class="number">64</span>-Bit Server VM, Java <span class="number">1.7</span>.<span class="number">0</span>_67)</span><br><span class="line"><span class="keyword">Type</span> <span class="keyword">in</span> expressions <span class="keyword">to</span> have them evaluated.</span><br><span class="line"><span class="keyword">Type</span> :help <span class="keyword">for</span> more information.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> WARN Utils: Your hostname, mymac resolves <span class="keyword">to</span> a loopback address: <span class="number">127.0</span>.<span class="number">0.1</span>; <span class="keyword">using</span> <span class="number">192.168</span>.<span class="number">3.7</span> instead (<span class="keyword">on</span> <span class="keyword">interface</span> en1)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> WARN Utils: <span class="keyword">Set</span> SPARK_LOCAL_IP <span class="keyword">if</span> you need <span class="keyword">to</span> bind <span class="keyword">to</span> another address</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: Changing view acls <span class="keyword">to</span>: dayongd</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: Changing modify acls <span class="keyword">to</span>: dayongd</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users <span class="keyword">with</span> view permissions: <span class="keyword">Set</span>(dayongd); users <span class="keyword">with</span> modify permissions: <span class="keyword">Set</span>(dayongd)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO Slf4jLogger: Slf4jLogger started</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO Remoting: Starting remoting</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO Remoting: Remoting started; listening <span class="keyword">on</span> addresses :[akka.tcp:<span class="comment">//sparkDriver@192.168.3.7:56841]</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO Utils: Successfully started service <span class="string">'sparkDriver'</span> <span class="keyword">on</span> port <span class="number">56841</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO DiskBlockManager: Created local directory at /<span class="keyword">var</span>/folders/zp/ns8pgfr91yj93hglw1mncph9ytq52s/T/spark-local-<span class="number">201501232021450</span>b71</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO MemoryStore: MemoryStore started <span class="keyword">with</span> capacity <span class="number">265.4</span> MB</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> WARN NativeCodeLoader: Unable <span class="keyword">to</span> load native-hadoop <span class="keyword">library</span> <span class="keyword">for</span> your <span class="keyword">platform</span>... <span class="keyword">using</span> builtin-java classes <span class="keyword">where</span> applicable</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO HttpFileServer: HTTP File server directory <span class="keyword">is</span> /<span class="keyword">var</span>/folders/zp/ns8pgfr91yj93hglw1mncph9ytq52s/T/spark-e7b15fc0-<span class="number">7</span>c73-<span class="number">4</span>ddb-<span class="number">9</span>c71-ffa1e83f255a</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO HttpServer: Starting HTTP Server</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Utils: Successfully started service <span class="string">'HTTP file server'</span> <span class="keyword">on</span> port <span class="number">56842</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Utils: Successfully started service <span class="string">'SparkUI'</span> <span class="keyword">on</span> port <span class="number">4040</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO SparkUI: Started SparkUI at http:<span class="comment">//192.168.3.7:4040</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Executor: <span class="keyword">Using</span> REPL <span class="keyword">class</span> URI: http:<span class="comment">//192.168.3.7:56839</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO AkkaUtils: Connecting <span class="keyword">to</span> HeartbeatReceiver: akka.tcp:<span class="comment">//sparkDriver@192.168.3.7:56841/user/HeartbeatReceiver</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO NettyBlockTransferService: Server created <span class="keyword">on</span> <span class="number">56843</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMaster: Trying <span class="keyword">to</span> <span class="keyword">register</span> BlockManager</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMasterActor: Registering <span class="keyword">block</span> manager localhost:<span class="number">56843</span> <span class="keyword">with</span> <span class="number">265.4</span> MB RAM, BlockManagerId(&lt;driver&gt;, localhost, <span class="number">56843</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMaster: Registered BlockManager</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">47</span> INFO SparkILoop: Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> sc.</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Note:">Note:</h3><p>If there is below exception, pls. make sure the local loop address is avaliable in the host file.  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.<span class="string">UnknownHostException:</span> <span class="string">mymac:</span> <span class="string">mymac:</span> nodename nor servname provided, or not known</span><br></pre></td></tr></table></figure>
<p>mymac$ sudo echo “127.0.0.1 mymac” &gt;&gt; /etc/hosts</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkmac.jpg" alt=""><br>It is great to see that Brew supports install Spark. It makes installation of Spark quite easi]]>
    </summary>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Java Programming Books Reviews]]></title>
    <link href="http://sparkera.ca/2015/01/01/reviews-java-programming/"/>
    <id>http://sparkera.ca/2015/01/01/reviews-java-programming/</id>
    <published>2015-01-01T13:01:11.000Z</published>
    <updated>2016-01-15T01:28:11.000Z</updated>
    <content type="html"><![CDATA[<p>Java is one of important programming language in big data ecosystem. Here, I recommand some great Java resource and books, which are very useful for big data development.<br><img src="http://whytoread.com/wp-content/uploads/2014/10/bestbooks-for-programming-java.jpg" alt=""></p>
<hr>
<p><img src="http://ecx.images-amazon.com/images/I/416oh7sATKL._SX331_BO1,204,203,200_.jpg" width="150" height="200" align="right"></p>
<h3 id="Java_Quick_Syntax_Reference_-_(The_Expert’s_Voice)"><a href="https://onlybooks.org/beginning-scala-2nd-edition-32119" target="_blank" rel="external">Java Quick Syntax Reference - (The Expert’s Voice)</a></h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>Read complete this book@Jan. 2015. The Java Quick Syntax Reference is a quite short book for your review Java sytax or learning java in a faster way. It is a condensed code and syntax reference with only 70 pages. In addition, it is not raw syntax but trying to explain lots of useful tips. </p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Java is one of important programming language in big data ecosystem. Here, I recommand some great Java resource and books, which are very]]>
    </summary>
    
      <category term="books" scheme="http://sparkera.ca/tags/books/"/>
    
      <category term="java" scheme="http://sparkera.ca/tags/java/"/>
    
      <category term="Reviews" scheme="http://sparkera.ca/categories/Reviews/"/>
    
      <category term="Coding" scheme="http://sparkera.ca/categories/Reviews/Coding/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Programming Books Reviews]]></title>
    <link href="http://sparkera.ca/2015/01/01/reviews-scala-programming/"/>
    <id>http://sparkera.ca/2015/01/01/reviews-scala-programming/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2016-01-05T00:13:02.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/beginscala.jpg" width="150" height="200" align="right"></p>
<h3 id="Begining_Scala_2nd_ed"><a href="https://onlybooks.org/beginning-scala-2nd-edition-32119" target="_blank" rel="external">Begining Scala 2nd ed</a></h3><p><img src="/images/3.5star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>Read complete this book@Oct. 2014. This book has very quick style of explaining scala with many detail examples. Even some examples has code defect and typo, it is easy to find out. The last part about scala best practice is really a valueable chapter. The other part is ok and cover everthing well enough but not deep enough.</p>
<hr>
<p><img src="http://akamaicovers.oreilly.com/images/0636920030287/lrg.jpg" width="150" height="200" align="right"></p>
<h3 id="Learning_Scala_Practical_Functional_Programming_for_the_JVM"><a href="http://shop.oreilly.com/product/0636920030287.do?sortby=publicationDate" target="_blank" rel="external">Learning Scala Practical Functional Programming for the JVM</a></h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>TRead complete this book@Nov. 2014. The book is my 1st book read about scala. It introduces the basic concet of scala as well as advanced topics. The last few chapters are little hard to read, but overall it is a great scala book having latest of scala features covered.</p>
<hr>
<p><img src="http://akamaicovers.oreilly.com/images/0636920026914/lrg.jpg" width="150" height="200" align="right"></p>
<h3 id="Scala_Cookbook"><a href="http://shop.oreilly.com/product/0636920026914.do" target="_blank" rel="external">Scala Cookbook</a></h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>The book is my 2nd book read about scala. This book is full of examples. It is more easier to learn thaning reading long chapters. I start reading it right now.</p>
<hr>
<p><img src="http://www.artima.com/images/puzzlersCover185x240.gif" width="150" height="200" align="right"></p>
<h3 id="Scala_Puzzlers"><a href="http://scalapuzzlers.com/" target="_blank" rel="external">Scala Puzzlers</a></h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>The book is my 3rd book read about scala. It is a free book in github and it is a fun book about scala. I start reading it right now.</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/beginscala.jpg" width="150" height="200" align="right"></p>
<h3 id="Begining_Scala_2nd_ed"><a href="https://onlybooks.o]]>
    </summary>
    
      <category term="books" scheme="http://sparkera.ca/tags/books/"/>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="Reviews" scheme="http://sparkera.ca/categories/Reviews/"/>
    
      <category term="Coding" scheme="http://sparkera.ca/categories/Reviews/Coding/"/>
    
  </entry>
  
</feed>
