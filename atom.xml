<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Sparkera]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://sparkera.ca/"/>
  <updated>2016-10-18T00:29:40.000Z</updated>
  <id>http://sparkera.ca/</id>
  
  <author>
    <name><![CDATA[Sparkera]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Apache Hive RowID Generation]]></title>
    <link href="http://sparkera.ca/2016/10/17/Apache-Hive-RowID-Generation/"/>
    <id>http://sparkera.ca/2016/10/17/Apache-Hive-RowID-Generation/</id>
    <published>2016-10-17T04:00:00.000Z</published>
    <updated>2016-10-18T00:29:40.000Z</updated>
    <content type="html"><![CDATA[<p><img src="http://www.codeinnovationsblog.com/wp-content/uploads/2016/08/big-data-hive.jpg" alt=""><br>It is quite often that we need a unique identifier for each single rows in the Apache Hive tables. This is quite useful when you need such columns as surrogate keys in data warehouse, as the primary key for data or use as system nature keys. There are following ways of doing that in Hive.</p>
<h3 id="ROW_NUMBER()">ROW_NUMBER()</h3><p>Hive have a couple of internal functions to achieve this. <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+WindowingAndAnalytics" target="_blank" rel="external">ROW_NUMBER</a> function, which can generate row number for each partition of data. Although there is no documentation in the official wiki, you can easily find articles regarding use this function to generate row number for the whole table.</p>
<pre><code><span class="type">SELECT</span> <span class="type">ROW_NUMBER</span><span class="literal">()</span> <span class="type">OVER</span> <span class="literal">()</span> <span class="keyword">as</span> row_num <span class="type">FROM</span> table_name
</code></pre><p>However, it is reported in some version of Hive that the function has exceptions when used in views or weird behavior, see <a href="https://issues.apache.org/jira/browse/HIVE-11583" target="_blank" rel="external">here</a> and <a href="https://issues.apache.org/jira/browse/HIVE-13072" target="_blank" rel="external">there</a>. Therefore, it is a quite danger to use this approach right now since your result is unexpected across different version of Hive.</p>
<h3 id="Java_Package">Java Package</h3><p>Hive is capable of leveraging Java internal packages naturally. By using Java’s UUID, you can directly generate a UUID number for each row as follows.</p>
<pre><code><span class="keyword">SELECT</span> 
regexp_replace(reflect(<span class="string">'java.util.UUID'</span>,<span class="string">'randomUUID'</span>), <span class="string">'-'</span>, <span class="string">''</span>) <span class="keyword">as</span> row_num  
<span class="keyword">FROM</span> table_name
</code></pre><p>As UUID generation logic defines, there are chance of collision. However, it is very low chance.</p>
<h3 id="Hive_Mall_Ex">Hive Mall Ex</h3><p>Hivemall is a scalable machine learning library that runs on Apache Hive. It has provided a UDF called <a href="https://github.com/myui/hivemall/blob/master/core/src/main/java/hivemall/tools/mapred/RowIdUDF.java" target="_blank" rel="external">RowIdUDF</a>. It combines a MapReduce task ID and random number as row</p>
<pre><code><span class="keyword">SELECT</span> rowid() <span class="keyword">as</span> row_num <span class="keyword">FROM</span> table_name
</code></pre><h3 id="Hive_Virtual_Columns">Hive Virtual Columns</h3><p>Hive has provided three <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VirtualColumns" target="_blank" rel="external">virtual columns</a> as follows.</p>
<pre><code>INPUT__FILE__NAME: Full URI <span class="operator">of</span> <span class="operator">a</span> <span class="built_in">file</span> name <span class="operator">in</span> HDFS.
BLOCK__OFFSET__INSIDE__FILE: Byte <span class="built_in">offset</span> <span class="keyword">for</span> <span class="operator">the</span> block <span class="built_in">from</span> beginning <span class="operator">of</span> <span class="operator">the</span> <span class="built_in">file</span>.
ROW__OFFSET__INSIDE__BLOCK: Byte <span class="built_in">offset</span> <span class="keyword">for</span> <span class="operator">the</span> <span class="built_in">line</span> <span class="operator">within</span> <span class="operator">the</span> block.
</code></pre><p>We can always combine three above virtual columns to identify the each line in the table. In addition, we can control the length of this combined value by increasing the number of files (increasing the number of reducers). As result, the offset value becomes smaller for each block. The usage of this approach is as follows. Note, ROW<strong>OFFSET</strong>INSIDE__BLOCK is only available for RCFile or SequenceFile format of data.</p>
<pre><code><span class="operator"><span class="keyword">SET</span> hive.exec.rowoffset=<span class="literal">true</span>;</span> // To enable ROW__OFFSET__INSIDE__BLOCK

<span class="operator">SELECT 
// <span class="keyword">Convert</span> <span class="keyword">file</span> <span class="keyword">name</span> <span class="keyword">as</span> <span class="built_in">number</span>, such <span class="keyword">as</span> <span class="number">000001</span>_0 <span class="keyword">to</span> <span class="number">10</span>
<span class="keyword">concat</span>(<span class="keyword">cast</span>(regexp_replace(<span class="keyword">reverse</span>(<span class="keyword">split</span>(<span class="keyword">reverse</span>(INPUT__FILE__NAME),<span class="string">'/'</span>)[<span class="number">0</span>]),<span class="string">'_'</span>,<span class="string">''</span>) <span class="keyword">as</span> <span class="built_in">int</span>), 
BLOCK__OFFSET__INSIDE__FILE, 
ROW__OFFSET__INSIDE__BLOCK) <span class="keyword">as</span> row_num
<span class="keyword">FROM</span> table_name</span>
</code></pre><h3 id="Conclusion">Conclusion</h3><p>Before Hive ROW_NUMBER() function issue gets fixed, it is recommended to use either Hivemall rowid() or virtual columns. If you need to control the length of row id, you have to use Hive virtual columns approach.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="http://www.codeinnovationsblog.com/wp-content/uploads/2016/08/big-data-hive.jpg" alt=""><br>It is quite often that we need a un]]>
    </summary>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Kafka Overview]]></title>
    <link href="http://sparkera.ca/2016/08/07/Apache-Kafka-Overview/"/>
    <id>http://sparkera.ca/2016/08/07/Apache-Kafka-Overview/</id>
    <published>2016-08-07T04:00:00.000Z</published>
    <updated>2016-08-07T15:06:51.000Z</updated>
    <content type="html"><![CDATA[<p>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, and Pig were all designed to process data in batches rather than continuously. Recently enterprises have discovered the power of analyzing and processing data and events as they happen instead of batches. Most traditional messaging systems, such as RabbitMq, neither scale up to handle big data in realtime nor use friendly with big data ecosystem. But, it is lucky that we have group of engineers at LinkedIn built and open-sourced distributed messaging framework that meets the demands of big data by scaling on commodity hardware, called <a href="http://kafka.apache.org/" target="_blank" rel="external">Kafka</a>.</p>
<h2 id="What_is_Kafka?">What is Kafka?</h2><p>Apache Kafka is messaging system built to scale for big data. Similar to Apache ActiveMQ or RabbitMq, Kafka enables applications built on different platforms to communicate via asynchronous message passing. But Kafka differs from these more traditional messaging systems in key ways:</p>
<ul>
<li>It’s designed to scale horizontally, by adding more commodity servers.</li>
<li>It provides much higher throughput for both producer and consumer processes.</li>
<li>It can be used to support both batch and real-time use cases.</li>
<li>It doesn’t support JMS, Java’s message-oriented middleware API.</li>
</ul>
<h2 id="Kafka’s_Architecture">Kafka’s Architecture</h2><p>There are some basic terminology used in Kafka architecture:</p>
<ul>
<li>A producer is process that can publish a message to a topic.</li>
<li>A consumer is a process that can subscribe to one or more topics and consume messages published to topics.</li>
<li>A topic category is the name of the feed to which messages are published.</li>
<li>A broker is a process running on single machine.</li>
<li>A cluster is a group of brokers working together.</li>
</ul>
<p><img src="http://hortonworks.com/wp-content/uploads/2014/10/Kafka-Broker-Diagram.png" alt=""><br>From above Kafka’s architecture, we can see it has a very simple design, which can result in better performance and throughput. Every topic in Kafka is like a simple log file. When a producer publishes a message, the Kafka server appends it to the end of the log file for its given topic. The server also assigns an offset, which is a number used to permanently identify each message. As the number of messages grows, the value of each offset increases; for example if the producer publishes three messages the first one might get an offset of 1, the second an offset of 2, and the third an offset of 3.</p>
<p>When the Kafka consumer first starts, it will send a pull request to the server, asking to retrieve any messages for a particular topic with an offset value higher than 0. The server will check the log file for that topic and return the three new messages. The consumer will process the messages, then send a request for messages with an offset higher than 3, and so on.</p>
<p>In Kafka, the client is responsible for remembering the offset count and retrieving messages.The Kafka server doesn’t track or manage message consumption. By default, a Kafka server will keep a message for seven days. A background thread in the server checks and deletes messages that are seven days or older. A consumer can access messages as long as they are on the server. It can read a message multiple times, and even read messages in reverse order of receipt. But if the consumer fails to retrieve the message before the seven days are up, it will miss that message.</p>
<h2 id="Why_Kafka_so_Quick">Why Kafka so Quick</h2><p>Kafka is fast for a number of reasons. Below are few aspects for your reference.</p>
<ul>
<li>Zero Copy: It calls the OS kernal direct rather than at the application layer to move data fast. Here is more explanation from <a href="https://en.wikipedia.org/wiki/Zero-copy" target="_blank" rel="external">wikipedia</a>.</li>
<li>Batch Data in Chunks - Kafka is all about batching the data into chunks. This minimises cross machine latency with all the buffering/copying that accompanies this.</li>
<li>Avoids Random Disk Access - as Kafka is an immutable commit log it does not need to rewind the disk and do many random I/O operations and can just access the disk in a sequential manner. This enables it to get similar speeds from a physical disk compared with memory.</li>
<li>Can Scale Horizontally - The ability to have thousands of partitions for a single topic spread among thousands of machines means Kafka can handle huge loads.</li>
</ul>
<h2 id="Future">Future</h2><p>At earlier of this year, Kafka has announced two important features brought by <a href="http://www.confluent.io/" target="_blank" rel="external">confluent</a>, which is the company started by that group of enginners in LinkedIn. One is Kafka Connect and the other is Kafka Stream. These two new “killing” features will be a big impact in the big data ecosystem. I’ll talk about them for more details in later posts.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>The big data processing started by focusing on the batch processing. Distributed data storage and querying tools like MapReduce, Hive, an]]>
    </summary>
    
      <category term="kafka" scheme="http://sparkera.ca/tags/kafka/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache HAWQ is Landing on HDP]]></title>
    <link href="http://sparkera.ca/2016/04/18/Apache-HAWQ-on-HDP/"/>
    <id>http://sparkera.ca/2016/04/18/Apache-HAWQ-on-HDP/</id>
    <published>2016-04-18T04:00:00.000Z</published>
    <updated>2016-04-19T01:47:09.000Z</updated>
    <content type="html"><![CDATA[<h2 id="News">News</h2><p>Last week, <a href="http://hortonworks.com/" target="_blank" rel="external">HDP</a> had announced to expend their strategic relationship with <a href="http://pivotal.io/" target="_blank" rel="external">Pivotal</a>. This will bring together Hortonworks’ expertise and support for data management and processing with Pivotal’s top analytics engine for Apache® HadoopTM. The full announcement is <a href="http://hortonworks.com/press-releases/hortonworks-pivotal-expand-relationship-deliver-enterprise-ready-modern-data-platforms-data-management-analytics/" target="_blank" rel="external">here</a>.</p>
<p>This is to say as I understand, Pivotal basically gives up their Pivotal HD version and mainly resell Hortonworks Data Platform (HDP) for instead. As the return, Hortonworks will support and include Pivotal’s leading product (open sourced last year) <a href="http://hawq.incubator.apache.org/" target="_blank" rel="external">Apache HAWQ</a> to the future version of HDP around Q2 of 2016. This is obviously a great deal for Hortonworks by converting a competitor to a friend of open source big data service provider. In addition, HDP finally gets a chance to include an MPP type of SQL over Hadoop solution in its distribution in order to well compete with Cloudera <a href="http://impala.io/" target="_blank" rel="external">Impala</a>. Nowadays, Apache Hive together with Apache Tez is still the leading tool for batch-oriented jobs. Considering the maturity as well as stability, this is not likely to change in short time. On the other hand, data analytics need more near-real time, more flexibility, and better performance access patterns rather than extrem high reliability (That is to say less frequent failures in the ad-hoc use cases are tolerable). As a result, Apache Impala + <a href="http://getkudu.io/" target="_blank" rel="external">Apache Kudu</a>, which are mainly contributed and distributed from Cloudera, starts catching more and more people’s attention. However, you are not likely to see them shipped in the HDP distribution because of the underlying competitions. However, the embrace of Apache HAWQ will change the game for Hortonworks.<br><img src="http://thenewstack.io/wp-content/uploads/2015/09/6122510599_f5d461ef18_z.jpg" alt=""></p>
<h2 id="History_-_MPP_vs-_Batch">History - MPP vs. Batch</h2><p>The design of MPP solutions is the shared-nothing architecture. Each MMP executor has separate CPU, memory, and disk resources. These resources are dedicated to the executors at run time. There may be a managed data exchange through the network, usually called synchronization which is required for shuffling the data across the cluster to perform joins and aggregations. The concept of resource isolation behind MPP solutions works perfectly fine most of the time except for two scenarios as follows.</p>
<ul>
<li><p>Performance bottlenecks: When data synchronization is required which is quite common for big data analytics, the low performed nodes in the MPP cluster will be the bottleneck of the job running. All of the other jobs may just wait for this slow job to complete so it brings down the whole cluster performance, even worse failed the query because of OOM issue. This is the same problem where MapReduce like algorithms try to solve through <a href="https://en.wikipedia.org/wiki/Speculative_execution" target="_blank" rel="external">speculative execution</a>.</p>
</li>
<li><p>Concurrency bottlenecks: There is known concurrency issue that MPP’s concurrency is tightly depending on the job instead of the scale of processing units/resources. For example, clusters of 50 nodes and 500 nodes would support the same level of concurrency. Here is an <a href="http://hortonworks.com/blog/impala-vs-hive-performance-benchmark/" target="_blank" rel="external">article</a> mentioning about the same issue in Apache Impala from Yahoo team. A modern MPP system always has such limitation of around 50 machines in a typical cluster without much more scalabilities. And, this is an area where MapReduce like algorithms can well leverage its independent task processing model and also scales well with more computing resource added.</p>
</li>
</ul>
<h2 id="Future">Future</h2><p>For the coming release of HDP with the closer strategic relationship with Pivotal, Apache HAWQ will introduce a completely new design with a combined advantages from both of MPP and Batch. Let’s look forward how it looks like soon :).</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="News">News</h2><p>Last week, <a href="http://hortonworks.com/" target="_blank" rel="external">HDP</a> had announced to expend their ]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hawq" scheme="http://sparkera.ca/tags/hawq/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Apply Method]]></title>
    <link href="http://sparkera.ca/2016/03/11/Scala-Apply-Method/"/>
    <id>http://sparkera.ca/2016/03/11/Scala-Apply-Method/</id>
    <published>2016-03-11T05:00:00.000Z</published>
    <updated>2016-03-12T03:14:36.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/apply_scala.png" alt=""><br>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics like java array access for an arbitrary class. </p>
<p>For example, we create a class of <code>RiceCooker</code> and its method <code>cook</code> to cook rice. Whenever we need to cook rice, we could call this method.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RiceCooker</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">cook</span>(</span>cup_of_rice: <span class="type">Rice</span>) = &#123;</span><br><span class="line">    cup_of_rice.isDone = <span class="literal">true</span></span><br><span class="line">    cup_of_rice</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">my_rice_cooker</span>:</span> <span class="type">RiceCooker</span> = <span class="keyword">new</span> <span class="type">RiceCooker</span>()</span><br><span class="line">my_rice_cooker.cook(<span class="keyword">new</span> <span class="type">Rice</span>())</span><br></pre></td></tr></table></figure></p>
<p>Since the main function of <code>RiceCooker</code> is to cook rice, we can always put this <code>cook</code> function in an <code>apply</code> method as follows. As result, we can cook immediately after we put rice in the rice cooker without explicit invoking any methods.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RiceCooker</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>cup_of_rice: <span class="type">Rice</span>) = &#123;</span><br><span class="line">    cup_of_rice.isDone = <span class="literal">true</span></span><br><span class="line">    cup_of_rice</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">val</span> <span class="title">my_rice_cooker</span>:</span> <span class="type">RiceCooker</span> = <span class="keyword">new</span> <span class="type">RiceCooker</span>()</span><br><span class="line">my_rice_cooker(<span class="keyword">new</span> <span class="type">Rice</span>())</span><br></pre></td></tr></table></figure></p>
<p>In addition, we can also move the apply method to the <a href="http://docs.scala-lang.org/tutorials/tour/singleton-objects.html" target="_blank" rel="external">companion object</a>. As result, the <code>apply</code> method becomes <strong>STATIC</strong>. Then, we can invoke it without creating a new <code>RiceCooker</code> object as below example.<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">RiceCooker</span> &#123;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(</span>cup_of_rice: <span class="type">Rice</span>) = &#123;</span><br><span class="line">    cup_of_rice.isDone = <span class="literal">true</span></span><br><span class="line">    cup_of_rice</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">RiceCooker</span>(<span class="keyword">new</span> <span class="type">Rice</span>())</span><br></pre></td></tr></table></figure></p>
<p>The best practice of using <code>apply</code> method in scala is to implement <a href="http://alvinalexander.com/scala/factory-pattern-in-scala-design-patterns" target="_blank" rel="external">factory pattern</a>. For example, the standard library of List in scala initializes objects using <code>apply</code> method in its companion object instead of using regular <code>new</code> keyword.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val a = List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>At the end, I give a comprehensive example of using the <code>apply</code> method in both way as follows.<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">class <span class="type">Test</span>(n :<span class="type">Int</span>) &#123;</span><br><span class="line">val a = <span class="type">Array</span>[<span class="type">Int</span>](n)</span><br><span class="line">def apply(n :<span class="type">Int</span>) = a(n)</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * <span class="type">Scala</span> simply makes a.update(x,y) to a(x)=y <span class="keyword">with</span> syntactic sugar</span><br><span class="line"> */</span><br><span class="line">def update(n:<span class="type">Int</span>, v:<span class="type">Int</span>) = a(n) = v </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> <span class="type">Test</span> &#123;</span><br><span class="line">def apply(n :<span class="type">Int</span>) = new <span class="type">Test</span>(n)</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * <span class="type">Call</span> <span class="keyword">static</span> apply <span class="keyword">method</span> <span class="keyword">in</span> companion <span class="keyword">object</span> to create an <span class="keyword">object</span></span><br><span class="line"> */</span><br><span class="line">val tester = <span class="type">Test</span>(<span class="number">10</span>) </span><br><span class="line"></span><br><span class="line">tester(<span class="number">0</span>) = <span class="number">1</span> //<span class="type">This</span> <span class="keyword">is</span> equal to tester.update(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">/*</span><br><span class="line"> * <span class="type">Call</span> a regular apply <span class="keyword">method</span> defined <span class="keyword">in</span> the class</span><br><span class="line"> * println(tester(<span class="number">0</span>)) <span class="keyword">is</span> equal to println(tester.apply(<span class="number">0</span>))</span><br><span class="line"> */</span><br><span class="line">println(tester(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/apply_scala.png" alt=""><br>The apply methods in scala has a nice syntactic sugar. It allows us to define semantics lik]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Develop Spark WordCount]]></title>
    <link href="http://sparkera.ca/2016/03/06/Dev-Spark-WordCount/"/>
    <id>http://sparkera.ca/2016/03/06/Dev-Spark-WordCount/</id>
    <published>2016-03-06T05:00:00.000Z</published>
    <updated>2016-03-07T02:24:25.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkwc.png" alt=""><br>It is quite often to setup Apache Spark development environment through IDE. Since I do not cover much setup IDE details in my Spark course, I am here to give detail steps for developing the well known Spark word count example using scala API in Eclipse.</p>
<h3 id="Environment">Environment</h3><ul>
<li><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">Apache Spark v1.6</a></li>
<li><a href="http://www.scala-lang.org/download/2.10.4.html" target="_blank" rel="external">Scala 2.10.4</a></li>
<li><a href="http://scala-ide.org/download/sdk.html" target="_blank" rel="external">Eclipse Scala IDE</a></li>
</ul>
<h3 id="Download_Software_Needed">Download Software Needed</h3><ol>
<li>Download the proper scala version and install it</li>
<li>Download the Eclipse scala IDE from above link</li>
</ol>
<h3 id="Create_Scala_Project">Create Scala Project</h3><ol>
<li><p>Open Scala Eclipse IDE. From the top menu, choose <code>File-&gt; New -&gt; Project -&gt; Maven project</code> with below information as example.</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Choose <span class="escape">`C</span>reat <span class="literal">a</span> simple project (skip archtype selection)<span class="escape">`</span><br><span class="line"></span>Choose <span class="escape">`U</span>se default Workspace location`</span><br></pre></td></tr></table></figure>
</li>
<li><p>Click <code>Next</code> button to go POM setting page and fill below information. Then, click <code>Finish</code>.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Group Id = ca<span class="class">.sparkera</span><span class="class">.spark</span> </span><br><span class="line">Artifact Id = WordCount</span><br></pre></td></tr></table></figure>
</li>
<li><p>Open the <code>pom.xml</code> file in eclipse working area and add replace using code <a href="/images/spark_wc_pom.xml">here</a>. Save the file, then Eclipse will automatically download the proper jar files and build the work space.</p>
</li>
<li><p>Add Scala Nature to this project by right clicking on <code>project -&gt; configure - &gt; Add Scala Nature</code>. </p>
</li>
<li><p>Update Scala compiler version for Spark by right clicking on <code>project- &gt; Properties -&gt; Scala Compiler -&gt; Use Project Settings -&gt;Scala Installation Latest 2.10 bundle (dynamic)</code>. </p>
</li>
<li><p>Refeactor source folder <code>src/main/java</code> to <code>src/main/scala</code> by <code>right click -&gt; Refactor -&gt; Rename.</code> Create a package under this name it as ca.sparkera.spark.</p>
</li>
<li><p>Create a Scala object under package created above package and name it as WordCount.scala by right clicking on the <code>package -&gt; New -&gt; Scala Object</code> and add WordCount as object name.</p>
</li>
<li><p>Paste below code as content for WordCount.scala</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> ca.sparkera.spark</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> &#123;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(</span>args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//check proper parameters - optional</span></span><br><span class="line">    <span class="keyword">if</span> (args.length &lt; <span class="number">1</span>) &#123;  </span><br><span class="line">      <span class="type">System</span>.err.println(<span class="string">"Usage: &lt;file&gt;"</span>)  </span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">1</span>)  </span><br><span class="line">    &#125;   </span><br><span class="line">    </span><br><span class="line">    <span class="comment">//Configuration for a Spark application.        </span></span><br><span class="line">    <span class="function"><span class="keyword">val</span> <span class="title">conf</span> =</span> <span class="keyword">new</span> <span class="type">SparkConf</span>()  </span><br><span class="line">    conf.setAppName(<span class="string">"SparkWordCount"</span>).setMaster(<span class="string">"local"</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//Create Spark Context  </span></span><br><span class="line">    <span class="function"><span class="keyword">val</span> <span class="title">sc</span> =</span> <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//Create MappedRDD by reading from HDFS file from path command line parameter  </span></span><br><span class="line">    <span class="function"><span class="keyword">val</span> <span class="title">rdd</span> =</span> sc.textFile(args(<span class="number">0</span>))  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//WordCount  </span></span><br><span class="line">    rdd.flatMap(_.split(<span class="string">" "</span>)).</span><br><span class="line">    map((_, <span class="number">1</span>)).</span><br><span class="line">    reduceByKey(_ + _).</span><br><span class="line">    map(x =&gt; (x._2, x._1)).</span><br><span class="line">    sortByKey(<span class="literal">false</span>).</span><br><span class="line">    map(x =&gt; (x._2, x._1)).</span><br><span class="line">    saveAsTextFile(<span class="string">"SparkWordCountResult"</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">//stop context</span></span><br><span class="line">    sc.stop  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run the application by right clicking on <code>WordCount.scala - &gt; Run as -&gt; Run Configurations -&gt; Arguments</code> and add input file path, such as <code>/Users/will/Downloads/testdata/</code></p>
</li>
<li><p>Check the output files containing word count result from proper place where you can find from console output.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s -l /Users/will/workspace/WordCount/SparkWordCountResult</span><br><span class="line">total <span class="number">1248</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> will  staff       <span class="number">0</span>  <span class="number">6</span> Mar <span class="number">16</span>:<span class="number">30</span> _SUCCESS</span><br><span class="line">-rw-r--r--  <span class="number">1</span> will  staff   <span class="number">42669</span>  <span class="number">6</span> Mar <span class="number">16</span>:<span class="number">30</span> part-<span class="number">00000</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> will  staff   <span class="number">15600</span>  <span class="number">6</span> Mar <span class="number">16</span>:<span class="number">30</span> part-<span class="number">00001</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> will  staff  <span class="number">129997</span>  <span class="number">6</span> Mar <span class="number">16</span>:<span class="number">30</span> part-<span class="number">00002</span></span><br><span class="line">-rw-r--r--  <span class="number">1</span> will  staff  <span class="number">443519</span>  <span class="number">6</span> Mar <span class="number">16</span>:<span class="number">30</span> part-<span class="number">00003</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Note">Note</h3><ul>
<li>Without using maven, we can alternatively add the downloaded spark assmeling jar file (at <code>../spark-1.6.0-bin-hadoop2.6/lib/spark-assembly-1.6.0-hadoop2.6.0.jar</code>) to the scala project build path as external jar libaray.</li>
<li>In real project, we usually export the above scala code as jar file, copy it to the spark cluster, and submit it using <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">spark-submit</a>.</li>
<li>Share a website to search <a href="http://mvnrepository.com/" target="_blank" rel="external">maven configurations</a></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkwc.png" alt=""><br>It is quite often to setup Apache Spark development environment through IDE. Since I do not cov]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Partially Applied Functions and Curry]]></title>
    <link href="http://sparkera.ca/2016/02/06/Scala-Partial-Applied-Function/"/>
    <id>http://sparkera.ca/2016/02/06/Scala-Partial-Applied-Function/</id>
    <published>2016-02-06T05:00:00.000Z</published>
    <updated>2016-02-12T01:18:09.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Partially_Applied_Functions">Partially Applied Functions</h3><p>When you invoke a scala function, you need to apply the arguments to the function. If you pass all the expected arguments, you say the function is fully applied. If you only send only a few of arguments, the function is called partially applied function. The partially applied function in scala brings us the convenience of binding some arguments and leaving the rest to be filled in later.</p>
<p>Below is an example of partially applied function in scala.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def <span class="function"><span class="title">sum</span><span class="params">(a:Int, b:Int, c:Int)</span></span> = <span class="tag">a</span> + <span class="tag">b</span> + c</span><br><span class="line">sum: (<span class="tag">a</span>: Int, <span class="tag">b</span>: Int, c: Int)Int</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="tag">var</span> sum1 = <span class="function"><span class="title">sum</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>, _:Int)</span></span></span><br><span class="line">sum1: Int =&gt; Int = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="title">sum1</span><span class="params">(<span class="number">3</span>)</span></span></span><br><span class="line">res0: Int = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="title">sum</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span></span></span><br><span class="line">res1: Int = <span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<p>In the above example, <code>sum1</code> is partially applied for the first 2 arguments. Whenever you want to omit all the argument in the function, such as doing var <code>sum1 = sum(_:Int, _:Int, _:Int)</code>, you can use <code>&lt;function&gt; _</code> to present as follows. Actually, partially applied function has an <code>apply</code> method, so you can leverate the short-cut function call.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var sum2 = sum _</span><br><span class="line">sum2: (Int, Int, Int) =&gt; Int = &lt;function3&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; sum2(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">res2: Int = <span class="number">6</span></span><br><span class="line"></span><br><span class="line">scala&gt; sum2.apply(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">res3: Int = <span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<p>Sometimes, we need to pay more attention when using overrided function as partially applied function as following example.<br><figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val k = println _</span><br><span class="line">k: () =&gt; <span class="type">Unit</span> = &lt;function0&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; k(<span class="number">3</span>)</span><br><span class="line">&lt;console&gt;:<span class="number">9</span>: error: too many arguments <span class="keyword">for</span> <span class="keyword">method</span> apply: ()<span class="type">Unit</span> <span class="keyword">in</span> trait <span class="type">Function0</span> k(<span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
<p>The reason we why <code>println _</code> is of type <code>() =&gt; Unit</code> instead of <code>Any =&gt; Unit</code> is because there exists an overloaded definition of println that doesn’t take any parameters in scala <a href="https://github.com/scala/scala/blob/v2.11.4/src/library/scala/Predef.scala#L309" target="_blank" rel="external">source code</a>:<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function"><span class="title">println</span><span class="params">()</span></span> = Console.<span class="function"><span class="title">println</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure></p>
<p>In order to use the println that takes a parameter, you have to explicitly tell the compiler to choose it as follows.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val k = <span class="function"><span class="title">println</span><span class="params">(_: Any)</span></span></span><br><span class="line">k: Any =&gt; Unit = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="title">k</span><span class="params">(<span class="number">3</span>)</span></span></span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p><img src="http://jvtrigueros.github.io/dcc4-function-composition-scala/images/curry.jpg" alt=""></p>
<h3 id="Currying_Functions">Currying Functions</h3><p>Currying, invented by Moses Schönfinkel and Gottlob Frege, is the technique of transforming a function that takes multiple arguments into a function that takes a single argument (the other arguments having been specified by the curry). Below is an example of function curry.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def <span class="function"><span class="title">add</span><span class="params">(x:Int)</span><span class="params">(y:Int)</span></span> = x + y</span><br><span class="line">add: (x: Int)(y: Int)Int</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="title">add</span><span class="params">(<span class="number">1</span>)</span><span class="params">(<span class="number">2</span>)</span></span></span><br><span class="line">res7: Int = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>In addition, an anonymous function returned from another function can be also curried as follows.<br><figure class="highlight zephir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def add2(x:<span class="keyword">Int</span>) = (y:<span class="keyword">Int</span>) =&gt; x + y</span><br><span class="line">add2: (x: <span class="keyword">Int</span>)<span class="keyword">Int</span> =&gt; <span class="keyword">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; add2(<span class="number">1</span>)(<span class="number">2</span>)</span><br><span class="line">res8: <span class="keyword">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>You can also convert a regular function to its curried version through partially applied function as following example. <code>add4</code> is a curried function from partially applied function <code>add3</code>.<br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; def add3(x:<span class="typename">Int</span>, y:<span class="typename">Int</span>) = x + y</span><br><span class="line">add3: (x: <span class="typename">Int</span>, y: <span class="typename">Int</span>)<span class="typename">Int</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="variable"><span class="keyword">val</span> add4</span>=(add3 _).curried</span><br><span class="line">add4: <span class="typename">Int</span> =&gt; (<span class="typename">Int</span> =&gt; <span class="typename">Int</span>) = <span class="type">&lt;function1&gt;</span></span><br><span class="line"></span><br><span class="line">scala&gt; add4(<span class="number">1</span>)(<span class="number">2</span>)</span><br><span class="line">res9: <span class="typename">Int</span> = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p>Most of time, we actually use in the other way by creating a specialized version of generalized function through curry function.<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="function">def <span class="title">filterBy</span><span class="params">(f: Int =&gt; Boolean)</span><span class="params">(xs: List[Int])</span> </span>= &#123;xs filter f&#125;</span><br><span class="line">filterBy: (f: Int =&gt; Boolean)(xs: List[Int])List[Int]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function">def <span class="title">even</span><span class="params">(x: Int)</span> </span>= x % <span class="number">2</span> == <span class="number">0</span></span><br><span class="line">even: (x: Int)Boolean</span><br><span class="line"></span><br><span class="line">scala&gt; val xs = List(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">12</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">xs: List[Int] = List(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">12</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; val ts = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">15</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line">ts: List[Int] = List(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">15</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; val evenFilter = filterBy(even) _</span><br><span class="line">evenFilter: List[Int] =&gt; List[Int] = &lt;function1&gt;</span><br><span class="line"></span><br><span class="line">scala&gt; evenFilter(xs)</span><br><span class="line">res10: List[Int] = List(<span class="number">4</span>, <span class="number">12</span>, <span class="number">6</span>)</span><br><span class="line">                      </span><br><span class="line">scala&gt; evenFilter(ts)</span><br><span class="line">res12: List[Int] = List(<span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure></p>
<p>For more tips about scala, please refer to the <a href="http://sparkera.ca/tags/scalatips/">scala tip</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Partially_Applied_Functions">Partially Applied Functions</h3><p>When you invoke a scala function, you need to apply the arguments to]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Call by Value vs. Name]]></title>
    <link href="http://sparkera.ca/2016/02/01/Scala-Func-Call/"/>
    <id>http://sparkera.ca/2016/02/01/Scala-Func-Call/</id>
    <published>2016-02-01T05:00:00.000Z</published>
    <updated>2016-01-30T14:12:33.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/scala.png" alt=""></p>
<p>From today, I start working on series of articles about how Scala is special and powerful than regular programming languages, such as Java, under tag <a href="http://sparkera.ca/tags/scalatips/">scalatips</a>. If you have any confused topic, please feel free to contact me so that I can try to use simple terms and explanations to help you understand it.</p>
<p>Usually, there are a couple of ways when calling functions, such as call-by-reference, call-by-value and call-by-name. </p>
<ol>
<li>call-by-reference: we are passing an alias of the variable to a function</li>
<li>call-by-value: we are passing the value of formula/variable to a function. </li>
<li>call-by-name: we are passing the whole formula to a function (there is no such saying when the argument is variable).</li>
</ol>
<p><strong>Note:</strong> For C, C++, they support 1. and 2. For Java, it only supports 2. For Scala, it supports 2, and 3.</p>
<p>Below is an example to demonstrate the difference in Scala.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//<span class="operator"><span class="keyword">Create</span> a printer utility <span class="keyword">for</span> demo. </span><br><span class="line"><span class="keyword">def</span> printer() = &#123;</span><br><span class="line"> println(<span class="string">"calling printer"</span>)</span><br><span class="line"> <span class="number">1</span> // <span class="keyword">return</span> <span class="keyword">value</span></span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">//This <span class="keyword">is</span> a <span class="keyword">call</span>-<span class="keyword">by</span>-<span class="keyword">value</span> syntax <span class="keyword">like</span> regular <span class="keyword">function</span></span><br><span class="line"><span class="keyword">def</span> callByValue(x: <span class="built_in">Int</span>) = &#123;</span><br><span class="line"> println(<span class="string">"x1="</span> + x)</span><br><span class="line"> println(<span class="string">"x2="</span> + x)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//This <span class="keyword">is</span> a <span class="keyword">call</span>-<span class="keyword">by</span>-<span class="keyword">name</span> syntax <span class="keyword">by</span> <span class="keyword">using</span> =&gt;</span><br><span class="line"><span class="keyword">def</span> callByName(x: =&gt; <span class="built_in">Int</span>) = &#123;</span><br><span class="line"> println(<span class="string">"x1="</span> + x)</span><br><span class="line"> println(<span class="string">"x2="</span> + x)</span><br><span class="line">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>Then, let’s try the difference by calling them one by one. The result is as follows.<br><figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; callByValue<span class="list">(<span class="keyword">printer</span><span class="list">()</span>)</span></span><br><span class="line">calling printer</span><br><span class="line">x1=1</span><br><span class="line">x2=1</span><br></pre></td></tr></table></figure></p>
<p>For above example, the function firstly evaluate the <code>printer()</code> function. As a result, it prints “calling printer” and value <code>1</code>. Then, the value <code>1</code> is assigned to the argument for function <code>callByValue</code>. Then, the <code>callByValue</code> function prints the x twice for x1 and x2.</p>
<figure class="highlight lisp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; callByName<span class="list">(<span class="keyword">printer</span><span class="list">()</span>)</span></span><br><span class="line">calling printer</span><br><span class="line">x1=1</span><br><span class="line">calling printer</span><br><span class="line">x2=1</span><br></pre></td></tr></table></figure>
<p>For above example, the function <code>printer</code> will pass into <code>x</code> as whole twice since the <code>x</code> is being called by <code>callByName</code> twice. As a result, the “calling printer” are shown twice.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/scala.png" alt=""></p>
<p>From today, I start working on series of articles about how Scala is special and powerful tha]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Use UDF in Spark DataFrame]]></title>
    <link href="http://sparkera.ca/2016/01/17/Use-UDF-Spark-DF/"/>
    <id>http://sparkera.ca/2016/01/17/Use-UDF-Spark-DF/</id>
    <published>2016-01-17T05:00:00.000Z</published>
    <updated>2016-01-17T18:33:08.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/spark_func.png" alt=""></p>
<p>It is very convenient to create, register, and use user define functions with data. In addition, the recent release of Apache Spark also supports writing user-defined aggregation functions <a href="https://issues.apache.org/jira/browse/SPARK-3947" target="_blank" rel="external">UDAF</a>. Below is a short piece of code to demonstrate creating and using UDF in spark shell. It is quite often that data engineers massage the data and create necessary toolkit functions. The data analyst or business analyst start using them through either Hive over spark, Spark SQL, or other BI tools.</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">//<span class="operator"><span class="keyword">Create</span> a <span class="keyword">case</span> <span class="keyword">class</span> struncture <span class="keyword">to</span> hold the <span class="keyword">data</span> <span class="keyword">for</span> demo. </span><br><span class="line">//You can also directly <span class="keyword">convert</span> Hive <span class="keyword">table</span> <span class="keyword">into</span> DF, </span><br><span class="line">//such <span class="keyword">as</span> sqlContext.<span class="keyword">table</span>(<span class="string">"hive_table_name"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="keyword">class</span> StockPrice(symbol:<span class="keyword">String</span>, price:Seq[<span class="keyword">Double</span>])</span><br><span class="line"></span><br><span class="line">//<span class="keyword">create</span> the demo <span class="keyword">data</span></span><br><span class="line">val <span class="keyword">data</span>=sc.parallelize(</span><br><span class="line">Seq(</span><br><span class="line">StockPrice(<span class="string">"APPL"</span>, Seq(<span class="number">93.5</span>, <span class="number">95.6</span>, <span class="number">102.7</span>)),</span><br><span class="line">StockPrice(<span class="string">"GOOG"</span>, Seq(<span class="number">604.5</span>, <span class="number">603.7</span>, <span class="number">614.1</span>)),</span><br><span class="line">StockPrice(<span class="string">"BABA"</span>, Seq(<span class="number">64.8</span>, <span class="number">95.2</span>, <span class="number">96.0</span>))</span><br><span class="line">)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Convert</span> the <span class="keyword">data</span> <span class="keyword">to</span> a DataFrame</span><br><span class="line">val df=<span class="keyword">data</span>.toDF(<span class="string">"symbol"</span>, <span class="string">"price"</span>)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Register</span> the results <span class="keyword">to</span> a temp <span class="keyword">table</span></span><br><span class="line">//Alternativelly, you can materilize the <span class="keyword">data</span> <span class="keyword">in</span> Hive <span class="keyword">by</span> </span><br><span class="line">//df.saveAsTable(<span class="string">"stock_price"</span>)</span><br><span class="line">df.registerTempTable(<span class="string">"stock_price"</span>)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Create</span> a <span class="keyword">function</span> that <span class="keyword">get</span><span class="string">'s the average race time</span><br><span class="line">def avgStockPrice(price:Double*) = &#123;</span><br><span class="line">var totalPriec=0.0</span><br><span class="line">price.foreach(x =&gt; totalPriec+=x)</span><br><span class="line">totalPriec/price.size</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">//Register that function with the SQLContext'</span>s UDF Registry</span><br><span class="line">//- means <span class="keyword">to</span> <span class="keyword">register</span> <span class="keyword">as</span> partital applied <span class="keyword">function</span></span><br><span class="line">sqlContext.udf.<span class="keyword">register</span>(<span class="string">"avgStockPrice"</span>, avgStockPrice _)</span><br><span class="line"></span><br><span class="line">//<span class="keyword">Use</span> the UDF <span class="keyword">in</span> a <span class="keyword">Query</span></span><br><span class="line">sqlContext.<span class="keyword">sql</span>(</span><br><span class="line"><span class="string">"select symbol, avgStockPrice(price) avg_stock_price</span><br><span class="line"> from stock_price"</span>).<span class="keyword">collect</span>().foreach(println)</span></span><br></pre></td></tr></table></figure>
<p>The result is as follows.<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[APPL,<span class="number">97.26666666666667</span>]</span><br><span class="line">[GOOG,<span class="number">607.4333333333334</span>]</span><br><span class="line">[BABA,<span class="number">85.33333333333333</span>]</span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/spark_func.png" alt=""></p>
<p>It is very convenient to create, register, and use user define functions with data. In a]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Happy New Year 2016]]></title>
    <link href="http://sparkera.ca/2016/01/01/Happy-New-Year-2016/"/>
    <id>http://sparkera.ca/2016/01/01/Happy-New-Year-2016/</id>
    <published>2016-01-01T05:00:00.000Z</published>
    <updated>2016-01-29T02:03:57.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/happy2016.jpg" alt=""><br>It is the end of 2015 and HAPPY NEW YEAR - 2016. It is time to wrap up my writing calendar with some summary on Sparkera, myself, and Big Data ecosystem.</p>
<p>In past 2015, I have published 21 articles in this blog, which finally got its name as SPARKERA - an era when a little spark can lead big fires. In this year, I also successfully migrated this blog from Jekyll to Hexo and renovate it with new UI and domain name. I also have my first book on Apache Hive published and start my career as an independent consultant for more challenge and exciting moment. In addition, I got more chances to share and teach my experience with many other people through various of courses, meetings, and talks. The year of 2015 is not easy but productive.</p>
<p>In 2015, there are also greats ideas and projects we cannot neglect in the big data ecosystem as follows.</p>
<ul>
<li><a href="http://kylin.apache.org/" target="_blank" rel="external">Apache Kylin</a> - is really an innovative idea by providing SQL interface and multi-dimensional analysis (OLAP) on Hadoop and HBase. This tool has more use cases for enterprise users and it is a better solution when the enterprise wants to build a data warehouse on top of Hadoop ecosystem.</li>
<li><a href="http://kafka.apache.org/" target="_blank" rel="external">Apache Kafaka</a> - a high-throughput distributed messaging system build for great scalability, high availability, ease of usage, as well as big data friendly.</li>
<li><a href="http://getkudu.io/" target="_blank" rel="external">Apache Kudu</a> - this is a smart solution by providing a super fast columnar storage having access pattern between HDFS and HBase. Kudu can well integrate with the Hadoop Ecosystem. Kudu is suitable for Data warehouse environment and interesting to see how it performs on columnar storage formats like Parquet, and ORC format.</li>
<li><a href="https://nifi.apache.org/" target="_blank" rel="external">Apache NiFi</a> - this is an easy to use, powerful, and reliable dataflow system. It has combined functionality between Apache Flume and ETL tools. HDP has integrated this tool in its latest distribution called Hortonworks Data Flow.</li>
<li>Apache HDFS <a href="https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">Cache</a> and <a href="https://issues.apache.org/jira/browse/HDFS-3107" target="_blank" rel="external">Truncate</a> - These two new features are expected for a long time to foresee Hadoop’s future strength as the core component in the big data ecosystem.</li>
<li>Apache YARN - new features supporting management label, long running jobs, and dock support</li>
<li><a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="external">Spark SQL and DataFrame</a> - Spark SQL brings Spark more closed to enterprise use case by leveraging the dataframe having schema and optimization on regular RDD. According to the benchmark, dataframe have great performance boost than regular RDD.</li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started" target="_blank" rel="external">Hive Over Spark</a> - makes Hive users/application to leverage powerful in-memory computing engine of Apache Spark.</li>
<li><a href="http://geode.incubator.apache.org/" target="_blank" rel="external">GemFire</a> and <a href="http://hawq.incubator.apache.org/" target="_blank" rel="external">HAWQ</a> - Very few enterprise would like to open source their leading products, but Pivotal did it and did for all its big data products. This is a great time to see the gap between enterprise ready products and open source softwares. This will also bring challenge and competitions for solutions, such as Impala, Hive, Spark SQL, HBase.</li>
<li><a href="https://zeppelin.incubator.apache.org/" target="_blank" rel="external">Apache Zeppelin</a> - There is very few open source visualization tools avaliable, especially which supporting many framework or system. Its native supports Apache Spark and interactive code-to-result experience attracting lots of community users.</li>
</ul>
<p>Well, again at the end, look forward a mazing and beautiful year of 2016 for me and you!</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/happy2016.jpg" alt=""><br>It is the end of 2015 and HAPPY NEW YEAR - 2016. It is time to wrap up my writing calendar wi]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Build Big Data Warehouse With Apache Hive]]></title>
    <link href="http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive%20copy/"/>
    <id>http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive copy/</id>
    <published>2015-12-21T01:50:58.000Z</published>
    <updated>2015-12-31T02:49:48.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-04apache-hive" title="Ten tools for ten big data areas 04_Apache Hive" target="_blank">Ten tools for ten big data areas 04_Apache Hive</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the fourth topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://hive.apache.org" target="_blank" rel="external">Apache Hive</a> is one of the earliest SQL on Hadoop approach. Although its legacy design is based on MapReduce, Hive involves fast and tries to be shining continually by providing sub-seconds query on top of Hadoop in future roadmap.</p>
<p>There are following areas where Hive could consider involving in the future.</p>
<ol>
<li><p>Dynamic Schema - Hive’s meta store provides a convenient view of schemaless data in the traditional schema view of data in RDBMS. This is a good approach for the usage transaction from the legacy database. But, there are increasing requirements for dynamically creating the schema which means we do not have to define the schema to access the data, especially for semi-structure and structure data. This is very useful when doing ad-hoc analysis. And, there are already other tools working in this way, such as pig, spark dataframe, apache drill, etc.</p>
</li>
<li><p>Smart Engine - There are two subareas where we define engine as smart, various and transparent.  For various, Hive has already supported working with different computing engines, such as MapReduce, Tez, and Spark. For transparent, we expect to see the switch between the different engine in a dynamic or even transparent way. There are always pros and cons for using different engines. If we can dynamic specify which engine to use by adding specific SQL keywords/hints, it will be an awesome feature for Apache Hive. As a result, we can use a single framework (hive) for the different use cases. Even smarter, the framework can pick up the best engine to run queries on the fly.</p>
</li>
<li><p>Multi-source Support. This is where we expect Hive can be a unified SQL over various of data sources so that we can easily to do data blending among different type of data sources.</p>
</li>
<li><p>Others -  Standard SQL-2011 support, store procedure like UDF, Live Long and Process (LLAP), leverage Hadoop <a href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">caching</a> features, metastore performance improvement, and advanced transaction supports. </p>
</li>
</ol>
<p><img src="/images/apachehive.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Light Big Data With Apache Spark]]></title>
    <link href="http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/"/>
    <id>http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/</id>
    <published>2015-12-20T02:10:58.000Z</published>
    <updated>2015-12-23T22:20:18.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-03apache-spark" title="Ten tools for ten big data areas 03_Apache Spark" target="_blank">Ten tools for ten big data areas 03_Apache Spark</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the third topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> is one of the greatest big data open source projects in nowadays. </p>
<p>There are so many articles, reports about how great the Apache Spark it is. I also heard lots of people keep saying that Spark will replace Hadoop and do everything in big data. As for me, I agree on some of them by saying Spark does a great job on some areas. However, I do disagree some saying about spark and I’ll talk about this today as most people do not realise there Spark may not work very well in some areas. </p>
<p>First, Spark is not going to replace Hadoop. Hadoop is a big data platform while Spark is an application. For another saying in terms of IPhone ecosystem, it is a relationship between IOS and Apps. Hadoop, especially Yarn, gradually become more and more important in the role of a platform by providing a multi-purpose universal platform for run various of big data applications. On the other hand, Spark is a powerful big data application which is able to do lots of things in big data. But it cannot “rules all” the big data ecosystem. We still have lots of use cases which require other big data applications. </p>
<p>Second, Spark is not good for everything. Below are some areas that I think we have other better options than using Spark.</p>
<ul>
<li><p>Spark has to rely on HDFS or other file systems to store data. It is a mainly computing engine. Spark is based on RDD, which is the immutable dataset. As a result, Spark does not fit for the use case where you need to modify the data. </p>
</li>
<li><p>Spark uses lots of micro-batch execution model to simulate data streaming. As a result, it has a limitation when the stream interval less than 0.5 seconds. For instead, you may need other truly real-time streaming framework, such as Apache <a href="http://storm.apache.org" target="_blank" rel="external">Storm</a> or <a href="http://flink.apache.org" target="_blank" rel="external">Flink</a>. </p>
</li>
<li><p>Spark runs on the JVM and leverages Java’s garbage collector. As JVM is designed for its general purpose, it lacks flexibility, good user experience, as well as efficient memory usage. Spark team has realised about this and comes up the Project Tungsten, which starts to build Spark’s own memory management system in the recent release. </p>
</li>
<li><p>For data ETL (extract, transformation, and load), you may not always need Spark’s speed, but focus more on the reliability as well as failure recovery. In this case, MapReduce’s processing style can be just fine as stable batch-mode processing. </p>
</li>
<li><p>Spark aims to replace MapReduce, but it does not provide any way to back compatible with old MapReduce jobs. However, there are legacy MapReduce jobs which could not be retired immediately from production.</p>
</li>
<li><p>MLLib in Spark still needs improvement by supporting more algorithm as well as accuracy.</p>
</li>
<li><p>GrapX in Spark is still new (comparing <a href="http://giraph.apache.org/" target="_blank" rel="external">Apache Giraph</a>) and some functions are only available in Scala API.</p>
</li>
</ul>
<p>Although there are areas to improve for Apache Spark, there is no doubt Apache Spark is a great big data application stack. I do believe it has good future. As there are big competition and fast evolvement in the big data ecosystem, let’s look forward to seeing if this little ‘spark’ can start a prairie fire.<br><img src="/images/apachespark.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Spark Memo]]></title>
    <link href="http://sparkera.ca/2015/12/12/Apache-Spark-Memo/"/>
    <id>http://sparkera.ca/2015/12/12/Apache-Spark-Memo/</id>
    <published>2015-12-12T14:04:58.000Z</published>
    <updated>2015-12-12T02:39:36.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benefit fot this collection.</p>
<h3 id="1-_Spark_Core">1. Spark Core</h3><ol>
<li><p>The stage creation rule is based on the idea to pipeline as many narrow transformations as possible. Once stages are figured out, spark will generate tasks from stages. The first stage will create ShuffleMapTasks and the last stage will create ResultTasks because in the last stage, one action operation is included to produce results.</p>
</li>
<li><p>The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks : one task per partition. Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition.</p>
</li>
<li><p>At high level, there are two transformations that can be applied onto the RDDs, namely narrow transformation and wide transformation. Wide transformations basically result in stage boundaries. <strong>Narrow transformation</strong> - doesn’t require the data to be shuffled across the partitions. for example, Map, filter and etc.. <strong>Wide transformation</strong> - requires the data to be shuffled for example, reduceByKey and etc..</p>
</li>
<li><p><strong>Partition</strong> - all the data you work with in Spark is split into partitions. What a single partition is and how is it determined? Partition size completely depends on the data source you use. For most of the methods to read the data in Spark you can specify the amount of partitions you want to have in your RDD. When you read a file from HDFS, you use Hadoop’s InputFormat to make it. By default each input split returned by InputFormat is mapped to a single partition in RDD. For most of the files on HDFS single input split is generated for a single block of data stored on HDFS, which equals to approximately 64MB of 128MB of data. Approximately, because the data in HDFS is split on exact block boundaries in bytes, but when it is processed it is split on the record splits. For text file the splitting character is the newline char, for sequence file it is the block end and so on. The only exception of this rule is compressed files – if you have the whole text file compressed, then it cannot be split into records and the whole file would become a single input split and thus a single partition in Spark and you have to manually repartition it.</p>
</li>
<li><p>Spark’s basic abstraction is the Resilient Distributed Dataset, or RDD. The RDD is how Spark simplifies complex operations like join or groupBy and hides the fact that under the hood, you’re dealing with fragmented data. That fragmentation is what enables Spark to execute in parallel, and the level of fragmentation is a function of the number of partitions of your RDD. The number of partitions is important because a stage in Spark will operate on one partition at a time (and load the data in that partition into memory). Consequently, if you have fewer partitions than active stages, you will wind up under-utilizing your cluster. Furthermore, since with fewer partitions there’s more data in each partition, you increase the memory pressure on your program. On the flip side, with too many partitions, your performance may degrade as you take a greater hit from network and disk I/O. Ultimately this concept ties into Spark’s notion of parallelism and how you can tune it (see the discussion of tuning parallelism here) to optimize performance.</p>
</li>
<li><p>The property <strong>spark.cleaner.ttl</strong> parameter to trigger automatic cleanups.</p>
</li>
<li><p>Keep in mind that repartitioning your data is a fairly expensive operation. Spark also has an optimized version of repartition() called coalesce() that allows minimum the data movement, but only if you are decreasing the number of RDD partitions.</p>
</li>
<li><p>In a typical Spark node，40% of the memory is used for computing while 60% is used for storing the data.</p>
</li>
<li><p><strong>cache()/persist()</strong> is also lays operation. However, unpersist() is non-lazy operation.</p>
</li>
</ol>
<h3 id="2-_Spark_SQL">2. Spark SQL</h3><ol>
<li><p>When working with a HiveContext, DataFrames can also be saved as persistent tables using the saveAsTable command. Unlike the registerTempTable command, saveAsTable will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SQLContext with the name of the table.</p>
</li>
<li><p>By default saveAsTable will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped. However, Spark can also create temp table from data frame using rdd.registerTempTable(“table_name”). These tables are out of control by hive.</p>
</li>
<li><p>Spark SQL also supports reading and writing data stored in Apache Hive. However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the -Phive and -Phive-thriftserver flags to Spark’s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benef]]>
    </summary>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Tableau Your Big Data]]></title>
    <link href="http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/"/>
    <id>http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/</id>
    <published>2015-12-12T02:04:58.000Z</published>
    <updated>2015-12-19T01:06:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-02tableau" title="Ten tools for ten big data areas 02_Tableau" target="_blank">Ten tools for ten big data areas 02_Tableau</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the second topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. <a href="http://www.tableau.com" target="_blank" rel="external">Tableau</a> is one of few commercial soft that I have to recommend in the visualization area for big data.</p>
<p>The term visualization becomes very popular when tableau software comes into the picture. As I remember, It was four or five years ago. When we first time to see how business intelligence can be like this and how data visualization looks like. Tableau software comes up the concept of self BI or BI without developers. This really scares me as I am a developer. But later, I have said it is correct. With the help of tableau, people can more focus on the meaning of data instead the old ways we deal with reporting. In addition, it is amazing fast speed and user experience makes it top notch in the data visualization domain and business intelligence. In recent years, we can see lots of traditional BI company start doing what was tableau doing, but it is clear it is far more away in terms of performance and sense of experience.</p>
<p><img src="/images/tableauyourdata.jpg" alt=""><br>Tableau is very first few of tools which provide connectors for big data from Hadoop Hive connector, impala, to Spark SQL. Tableau also announced new direct-connection capabilities with InfoSphere BigInsights from IBM, along with new beta connectors for Amazon Elastic MapReduce from Amazon Web Services Inc.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="tableau" scheme="http://sparkera.ca/tags/tableau/"/>
    
      <category term="visualization" scheme="http://sparkera.ca/tags/visualization/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ten Tools for Ten Big Data Areas]]></title>
    <link href="http://sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/"/>
    <id>http://sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/</id>
    <published>2015-11-28T21:04:58.000Z</published>
    <updated>2015-12-21T01:25:25.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special magic and power. Anyone who can own one of these weapons could become a master or leader who is not undefeatable.<br><img src="/images/10weapons.png" alt=""></p>
<p>Nowadays, the big data ecosystem becomes bigger and bigger. There are thousands of players in the big data landscape actively paying right now by creating thousands of tools, framework, solution, etc. However, I believe the legend are always among few of them. I choose 10 of the greatest tools as my recommendation for people really want to learn or use big data (see headline picture). </p>
<p>I have started the journey to introduce each of them which are free or not, but I guarantee the best from my experience. Here is the tag <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a> for this series of presentations as well as direct link for each of them. Let’s look forward all of them in future…</p>
<ol>
<li><a href="http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/">Informatica in Big Data</a></li>
<li><a href="http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/">Tableau Your Big Daya</a></li>
<li><a href="http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/">Light Big Data with Apache Spark</a></li>
<li><a href="http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive/">Build Big Data Warehouse with Apache Hive</a></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special ]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://sparkera.ca/tags/bigdata/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Informatica in Big Data]]></title>
    <link href="http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/"/>
    <id>http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/</id>
    <published>2015-10-25T03:04:58.000Z</published>
    <updated>2015-12-19T01:09:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-01-informatica-54334595" title="Ten tools for ten big data areas 01 informatica " target="_blank">Ten tools for ten big data areas 01 informatica </a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the first topic I’ll cover for series of talks about the <a href="http://sparkera.ca/tags/10t10a/">Ten Tools for Ten Big Data Areas</a>. I came up this series of topics from ten artifacts from ancient China when people are honored to get one of the ten artifacts.</p>
<p>In terms of data integrations of big data, I pick up Informatica, which is used to be a public company in NASDAQ as INFA. In the middle of this year, Informatica announced the successful completion of its acquisition by a company controlled by the Permira funds and Canada Pension Plan Investment Board (CPPIB). Additionally Informatica announced that Microsoft Corporation and Salesforce Ventures have agreed to become strategic investors in the company alongside the Permira funds and CPPIB. The acquisition is valued at approximately $5.3 billion, with Informatica stockholders receiving $48.75 in cash per share.</p>
<p>Especially in big data area, Informatica has leading product called Informatica big data edition - developer. This is brand new tool in the Informatica family. It has new user interface based on Eclipse. It is single tool including development all ETL job components. In Informatica developer, this is no more session. For instead, there is no concept called application. We can add mapping or workflow in the application to deploy and run the bulk of ETL jobs as an application.</p>
<p>The main advantage of Informatica developer is that it converts the ETL logic/mapping into Hive query and execute it on top of Hadoop cluster. For example, you can even push a none-Hadoop related jobs running on top of Hadoop. This advantage not only make ETL job leverages the power of computing resource of Hadoop but also get rid of additional budget for a dedicated ETL server cluster like what’s in the Informatica PowerCenter period. In addition, this design has lots of potential when Hive evolves in the big data ecosystem, such as Hive over Spark. In future, Informatica developer will be able to leverage more distributed computing framework beyond of Hadoop, such as Spark for better performance.</p>
<p>However, there are still limitations for this tool which is still new in the Informatica family. Below are some limitations I come across recently.</p>
<ul>
<li>Does not support Hive table in complex formats, such as Avro, etc</li>
<li>Does not support write into buckets tables</li>
<li>Does not support using parameters in row level, complex data objects path.</li>
<li>Does not support to return target successful or failed rows from mapping</li>
<li>Cannot run the workflow or application straightforward in the developer tool</li>
<li>None of errors and exceptions are reported at run time when running in Hive mode</li>
<li>Overall reliability need to be improved, such as OOM, exception on data adapters</li>
</ul>
<p>Alternatives, there are also other ETL tools for choice. <a href="https://www.talend.com/" target="_blank" rel="external">Talend</a> and <a href="http://www.pentaho.com/" target="_blank" rel="external">Pentaho</a> all provide big data version of tools. But, their big data version almost the same to the regular one except having common big data tool connector shipped. Therefore, these tools can read or write data between Hadoop rather than running on top of Hadoop.</p>
<p>There is another new data flow tool which people may pay attention, <a href="https://nifi.apache.org/" target="_blank" rel="external">Apache NiFi</a>, which supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. The company behind this tool is called Onyara, an early-stage startup acquired by Hontonworks in Auguest of 2015.<br><img src="/images/bde.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://sparkera.ca/tags/10t10a/"/>
    
      <category term="informatica" scheme="http://sparkera.ca/tags/informatica/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cloudera Launches One Platform Initatives To Advance Spark]]></title>
    <link href="http://sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/"/>
    <id>http://sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/</id>
    <published>2015-09-23T15:27:57.000Z</published>
    <updated>2015-09-24T02:56:23.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.</p>
<p>The Spark is originally invented by few guys who started up the <a href="https://databricks.com/" target="_blank" rel="external">Databrick</a>. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn. As a result, more and more companies start switching their MapReduce jobs to Spark and few of them already have big cluster deployed in production. Few months earlier, IBM has claimed that they would have 5000+ developers working in Apache Spark to make it better (It is heard that the core DB2 development force are reassigned to this new mission inside the IBM, not sure.). Clearly, the One Platform initiative is an echo for IBM’s saying from Cloudera who always believe itself a leader in the domain in big data. Cloudera is not likely to leave IBM alone to take this delicious fruit - Spark. We are waiting for more actions from other company, such as Hortonworks, MapR. I do not believe they just keep silent. Or maybe there is the underlying discussion to acquire the Databrick, who knows.</p>
<p>The One Platform initiative has covers four areas of efforts including security, scale, management, streaming. For more information regarding the One Platform initiative, please refer to the <a href="http://vision.cloudera.com/one-platform/" target="_blank" rel="external">Cloudera post</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that]]>
    </summary>
    
      <category term="spark" scheme="http://sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Get Git Modified But Untracked Content Checked In]]></title>
    <link href="http://sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/"/>
    <id>http://sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/</id>
    <published>2015-08-21T15:27:57.000Z</published>
    <updated>2015-08-22T18:11:54.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the <code>git add .</code> and <code>git status</code>. It shows below error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit. I only realize that I miss the theme files when I try to rebuild the Hexo site from home.<br><img src="/images/gituntracked.png" alt=""><br>After searching a while form Google, I got my issues resolved and share the steps below for reference.</p>
<ul>
<li>Removed the .git directories from the directories (In my case, ../theme/hueman/.git)</li>
<li>Ran git rm -rf –cached <the untracked="" directory=""> (In my case, /C/Users/ddu/Git/sparkera/themes/hueman)</the></li>
<li>Re-added the directories with a <code>git add .</code> and check by <code>git status</code>.</li>
</ul>
<p>Then all the untracked files are added. Then you can do <code>git commit -m</code> and <code>git push</code> the submit all the changes.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo p]]>
    </summary>
    
      <category term="git" scheme="http://sparkera.ca/tags/git/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop Streaming]]></title>
    <link href="http://sparkera.ca/2015/06/21/Hadoop%20Streaming/"/>
    <id>http://sparkera.ca/2015/06/21/Hadoop Streaming/</id>
    <published>2015-06-21T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic API which allows writing Mappers and Reduces in any language. </p>
<ul>
<li>Develop MapReduce jobs in practically any language</li>
<li>Uses Unix Streams as communication mechanism between Hadoop and your code</li>
<li>Any language that can read standard input and write are supported</li>
</ul>
<p>Few good use-cases:</p>
<ul>
<li>Text processing - scripting languages do well in text analysis</li>
<li>Utilities and/or expertise in languages other than Java</li>
</ul>
<h4 id="2-_Process_Flow">2. Process Flow</h4><p>Below is how streaming processing</p>
<ul>
<li>Map input passed over standard input</li>
<li>Map processes input line-by-line</li>
<li>Map writes output to standard output - Key-value separate by tab</li>
<li>Reduce input passed over standard input<ul>
<li>Same as mapper output – key-value pairs separated by tab</li>
<li>Input is sorted by key</li>
</ul>
</li>
<li>Reduce writes output to standard output</li>
</ul>
<p><img src="/images/hadoopstreaming.png" alt="&quot;avatar&quot;"></p>
<h4 id="3-_Example_of_mapper">3. Example of mapper</h4><p><strong>mapper.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python </span></span><br><span class="line">import sys </span><br><span class="line"><span class="comment"># mapper.py </span></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input) </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>: </span><br><span class="line"><span class="comment"># remove leading and trailing white space </span></span><br><span class="line"><span class="built_in">line</span> = <span class="built_in">line</span>.strip() </span><br><span class="line"><span class="comment"># split the line into words </span></span><br><span class="line"><span class="keyword">words</span> = <span class="built_in">line</span>.<span class="built_in">split</span>() </span><br><span class="line"><span class="comment"># increase counters for word in words: </span></span><br><span class="line"><span class="comment"># write the results to STDOUT (standard output); </span></span><br><span class="line"><span class="comment"># what we output here will be the input for the </span></span><br><span class="line"><span class="comment"># Reduce step, i.e. the input for reducer.py </span></span><br><span class="line"><span class="comment"># tab-delimited; the trivial word count is 1 </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">word</span> <span class="operator">in</span> <span class="keyword">words</span></span><br><span class="line">print <span class="string">'%s\t%s'</span> % (<span class="built_in">word</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="4-_Example_of_reducer">4. Example of reducer</h4><p><strong>reducer.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#reducer.py</span></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">current_word = None</span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line"><span class="built_in">word</span> = None</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>:</span><br><span class="line">    <span class="comment"># remove leading and trailing white space</span></span><br><span class="line">    <span class="built_in">line</span> = <span class="built_in">line</span>.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    <span class="built_in">word</span>, count = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">'\t'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = int(count)</span><br><span class="line">    except ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently ignore/discard this line</span></span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch works because Hadoop sorts map output by key before passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = <span class="built_in">word</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">    print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure>
<h4 id="5-_Run_the_job">5. Run the job</h4><ul>
<li>Test in local mode from Linux pipe</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat testText.txt | mapper.py | sort | reducer.py</span><br><span class="line">a <span class="number">1</span></span><br><span class="line">h <span class="number">1</span></span><br><span class="line">i <span class="number">4</span></span><br><span class="line">s <span class="number">1</span></span><br><span class="line">t <span class="number">5</span></span><br><span class="line">v <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Run in the cluster</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop/yarn jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-*<span class="class">.jar</span> \</span><br><span class="line">-D mapred<span class="class">.job</span><span class="class">.name</span>=<span class="string">"Count Job via Streaming"</span> \</span><br><span class="line">-files <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/mapper<span class="class">.py</span>, <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/reducer<span class="class">.py</span> \</span><br><span class="line">-<span class="tag">input</span> /training/input/hamlet<span class="class">.txt</span> \</span><br><span class="line">-output /training/output/ \</span><br><span class="line">-mapper mapper<span class="class">.py</span> \</span><br><span class="line">-combiner reducer<span class="class">.py</span> \</span><br><span class="line">-reducer reducer.py</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic]]>
    </summary>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Constructor vs. Java Constructor]]></title>
    <link href="http://sparkera.ca/2015/04/20/Scala%20and%20Java%20Constructors/"/>
    <id>http://sparkera.ca/2015/04/20/Scala and Java Constructors/</id>
    <published>2015-04-20T04:00:00.000Z</published>
    <updated>2016-01-30T02:14:53.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="2-_Constructor_With_Private_Attribute">2. Constructor With Private Attribute</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> Bar bar;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="keyword">private</span> <span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="3-_Call_Super_Constructor">3. Call <em>Super</em> Constructor</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() <span class="keyword">extends</span> <span class="title">SuperFoo</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;   </span><br><span class="line">      <span class="keyword">super</span>(bar);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="title">bar</span>:<span class="title">Bar</span>) <span class="keyword">extends</span> <span class="title">SuperFoo</span>(<span class="title">bar</span>) </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-_Multiple_Constructors">4. Multiple Constructors</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span> &#123;  </span><br><span class="line">    <span class="keyword">public</span> Bar bar;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params"></span>) </span>&#123;   </span><br><span class="line">       <span class="keyword">this</span>(<span class="keyword">new</span> Bar());   </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params">Bar bar</span>) </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>. bar = bar;   </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(</span><span class="function"><span class="keyword">val</span> <span class="title">bar</span>:</span><span class="type">Bar</span>)&#123;  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>) = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">Bar</span>)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-_Methods_of_getter_and_setter">5. Methods of <em>getter</em> and <em>setter</em></h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125; </span><br><span class="line">   <span class="function"><span class="keyword">public</span> Bar <span class="title">getBar</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">return</span> bar;   </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBar</span><span class="params">(Bar bar)</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>1. Scala Code</strong></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="import"><span class="keyword">import</span> scala.reflect._  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Foo</span><span class="container">(@<span class="type">BeanProperty</span> <span class="title">var</span> <span class="title">bar</span>:<span class="type">Bar</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>2. Scala Code</strong></p>
<pre><code><span class="keyword">import</span> scala.reflect._  
<span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(aBar:Bar) {  
    @BeanProperty  
    <span class="keyword">private</span> <span class="variable"><span class="keyword">var</span> bar</span> = aBar  
}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Ja]]>
    </summary>
    
      <category term="scala" scheme="http://sparkera.ca/tags/scala/"/>
    
      <category term="scalatips" scheme="http://sparkera.ca/tags/scalatips/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Hive Essentials Published]]></title>
    <link href="http://sparkera.ca/2015/03/15/Apache%20Hive%20Essentials%20Published/"/>
    <id>http://sparkera.ca/2015/03/15/Apache Hive Essentials Published/</id>
    <published>2015-03-15T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<li>My very first book</li>
<li>Also the first book on Apache Hive 1.0.0 in the world</li>
</ul>
<p>Check it out <a href="http://bit.ly/1LRkd5m" target="_blank" rel="external">here</a></p>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/apache-hive-essentials" target="_blank"><img src="/images/hivebooks.jpg" width="150" height="200" align="left"></a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<]]>
    </summary>
    
      <category term="hadoop" scheme="http://sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://sparkera.ca/categories/Blog/"/>
    
  </entry>
  
</feed>
