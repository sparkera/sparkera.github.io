<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Sparkera]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://www.sparkera.ca/"/>
  <updated>2015-12-31T02:49:48.000Z</updated>
  <id>http://www.sparkera.ca/</id>
  
  <author>
    <name><![CDATA[Sparkera]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Build Big Data Warehouse With Apache Hive]]></title>
    <link href="http://www.sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive/"/>
    <id>http://www.sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive/</id>
    <published>2015-12-21T01:50:58.000Z</published>
    <updated>2015-12-31T02:49:48.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-04apache-hive" title="Ten tools for ten big data areas 04_Apache Hive" target="_blank">Ten tools for ten big data areas 04_Apache Hive</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the fourth topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/" target="_blank" rel="external">Ten Tools for Ten Big Data Areas</a>. <a href="http://hive.apache.org" target="_blank" rel="external">Apache Hive</a> is one of the earliest SQL on Hadoop approach. Although its legacy design is based on MapReduce, Hive involves fast and tries to be shining continually by providing sub-seconds query on top of Hadoop in future roadmap.</p>
<p>There are following areas where Hive could consider involving in the future.</p>
<ol>
<li><p>Dynamic Schema - Hive’s meta store provides a convenient view of schemaless data in the traditional schema view of data in RDBMS. This is a good approach for the usage transaction from the legacy database. But, there are increasing requirements for dynamically creating the schema which means we do not have to define the schema to access the data, especially for semi-structure and structure data. This is very useful when doing ad-hoc analysis. And, there are already other tools working in this way, such as pig, spark dataframe, apache drill, etc.</p>
</li>
<li><p>Smart Engine - There are two subareas where we define engine as smart, various and transparent.  For various, Hive has already supported working with different computing engines, such as MapReduce, Tez, and Spark. For transparent, we expect to see the switch between the different engine in a dynamic or even transparent way. There are always pros and cons for using different engines. If we can dynamic specify which engine to use by adding specific SQL keywords/hints, it will be an awesome feature for Apache Hive. As a result, we can use a single framework (hive) for the different use cases. Even smarter, the framework can pick up the best engine to run queries on the fly.</p>
</li>
<li><p>Multi-source Support. This is where we expect Hive can be a unified SQL over various of data sources so that we can easily to do data blending among different type of data sources.</p>
</li>
<li><p>Others -  Standard SQL-2011 support, store procedure like UDF, Live Long and Process (LLAP), leverage Hadoop <a href="http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/CentralizedCacheManagement.html" target="_blank" rel="external">caching</a> features, metastore performance improvement, and advanced transaction supports. </p>
</li>
</ol>
<p><img src="/images/apachehive.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/ADqZ9PRgLL4NgB" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://www.sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="hive" scheme="http://www.sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Light Big Data With Apache Spark]]></title>
    <link href="http://www.sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/"/>
    <id>http://www.sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/</id>
    <published>2015-12-20T02:10:58.000Z</published>
    <updated>2015-12-23T22:20:18.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-03apache-spark" title="Ten tools for ten big data areas 03_Apache Spark" target="_blank">Ten tools for ten big data areas 03_Apache Spark</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the third topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/" target="_blank" rel="external">Ten Tools for Ten Big Data Areas</a>. <a href="http://spark.apache.org" target="_blank" rel="external">Apache Spark</a> is one of the greatest big data open source projects in nowadays. </p>
<p>There are so many articles, reports about how great the Apache Spark it is. I also heard lots of people keep saying that Spark will replace Hadoop and do everything in big data. As for me, I agree on some of them by saying Spark does a great job on some areas. However, I do disagree some saying about spark and I’ll talk about this today as most people do not realise there Spark may not work very well in some areas. </p>
<p>First, Spark is not going to replace Hadoop. Hadoop is a big data platform while Spark is an application. For another saying in terms of IPhone ecosystem, it is a relationship between IOS and Apps. Hadoop, especially Yarn, gradually become more and more important in the role of a platform by providing a multi-purpose universal platform for run various of big data applications. On the other hand, Spark is a powerful big data application which is able to do lots of things in big data. But it cannot “rules all” the big data ecosystem. We still have lots of use cases which require other big data applications. </p>
<p>Second, Spark is not good for everything. Below are some areas that I think we have other better options than using Spark.</p>
<ul>
<li><p>Spark has to rely on HDFS or other file systems to store data. It is a mainly computing engine. Spark is based on RDD, which is the immutable dataset. As a result, Spark does not fit for the use case where you need to modify the data. </p>
</li>
<li><p>Spark uses lots of micro-batch execution model to simulate data streaming. As a result, it has a limitation when the stream interval less than 0.5 seconds. For instead, you may need other truly real-time streaming framework, such as Apache <a href="http://storm.apache.org" target="_blank" rel="external">Storm</a> or <a href="http://flink.apache.org" target="_blank" rel="external">Flink</a>. </p>
</li>
<li><p>Spark runs on the JVM and leverages Java’s garbage collector. As JVM is designed for its general purpose, it lacks flexibility, good user experience, as well as efficient memory usage. Spark team has realised about this and comes up the Project Tungsten, which starts to build Spark’s own memory management system in the recent release. </p>
</li>
<li><p>For data ETL (extract, transformation, and load), you may not always need Spark’s speed, but focus more on the reliability as well as failure recovery. In this case, MapReduce’s processing style can be just fine as stable batch-mode processing. </p>
</li>
<li><p>Spark aims to replace MapReduce, but it does not provide any way to back compatible with old MapReduce jobs. However, there are legacy MapReduce jobs which could not be retired immediately from production.</p>
</li>
<li><p>MLLib in Spark still needs improvement by supporting more algorithm as well as accuracy.</p>
</li>
<li><p>GrapX in Spark is still new (comparing <a href="http://giraph.apache.org/" target="_blank" rel="external">Apache Giraph</a>) and some functions are only available in Scala API.</p>
</li>
</ul>
<p>Although there are areas to improve for Apache Spark, there is no doubt Apache Spark is a great big data application stack. I do believe it has good future. As there are big competition and fast evolvement in the big data ecosystem, let’s look forward to seeing if this little ‘spark’ can start a prairie fire.<br><img src="/images/apachespark.png" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/vSpgCYWEwBsJc7" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://www.sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://www.sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Spark Memo]]></title>
    <link href="http://www.sparkera.ca/2015/12/12/Apache-Spark-Memo/"/>
    <id>http://www.sparkera.ca/2015/12/12/Apache-Spark-Memo/</id>
    <published>2015-12-12T14:04:58.000Z</published>
    <updated>2015-12-12T02:39:36.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benefit fot this collection.</p>
<h3 id="1-_Spark_Core">1. Spark Core</h3><ol>
<li><p>The stage creation rule is based on the idea to pipeline as many narrow transformations as possible. Once stages are figured out, spark will generate tasks from stages. The first stage will create ShuffleMapTasks and the last stage will create ResultTasks because in the last stage, one action operation is included to produce results.</p>
</li>
<li><p>The number of tasks to be generated depends on how your files are distributed. Suppose that you have 3 three different files in three different nodes, the first stage will generate 3 tasks : one task per partition. Therefore, you should not map your steps to tasks directly. A task belongs to a stage, and is related to a partition.</p>
</li>
<li><p>At high level, there are two transformations that can be applied onto the RDDs, namely narrow transformation and wide transformation. Wide transformations basically result in stage boundaries. <strong>Narrow transformation</strong> - doesn’t require the data to be shuffled across the partitions. for example, Map, filter and etc.. <strong>Wide transformation</strong> - requires the data to be shuffled for example, reduceByKey and etc..</p>
</li>
<li><p><strong>Partition</strong> - all the data you work with in Spark is split into partitions. What a single partition is and how is it determined? Partition size completely depends on the data source you use. For most of the methods to read the data in Spark you can specify the amount of partitions you want to have in your RDD. When you read a file from HDFS, you use Hadoop’s InputFormat to make it. By default each input split returned by InputFormat is mapped to a single partition in RDD. For most of the files on HDFS single input split is generated for a single block of data stored on HDFS, which equals to approximately 64MB of 128MB of data. Approximately, because the data in HDFS is split on exact block boundaries in bytes, but when it is processed it is split on the record splits. For text file the splitting character is the newline char, for sequence file it is the block end and so on. The only exception of this rule is compressed files – if you have the whole text file compressed, then it cannot be split into records and the whole file would become a single input split and thus a single partition in Spark and you have to manually repartition it.</p>
</li>
<li><p>Spark’s basic abstraction is the Resilient Distributed Dataset, or RDD. The RDD is how Spark simplifies complex operations like join or groupBy and hides the fact that under the hood, you’re dealing with fragmented data. That fragmentation is what enables Spark to execute in parallel, and the level of fragmentation is a function of the number of partitions of your RDD. The number of partitions is important because a stage in Spark will operate on one partition at a time (and load the data in that partition into memory). Consequently, if you have fewer partitions than active stages, you will wind up under-utilizing your cluster. Furthermore, since with fewer partitions there’s more data in each partition, you increase the memory pressure on your program. On the flip side, with too many partitions, your performance may degrade as you take a greater hit from network and disk I/O. Ultimately this concept ties into Spark’s notion of parallelism and how you can tune it (see the discussion of tuning parallelism here) to optimize performance.</p>
</li>
<li><p>The property <strong>spark.cleaner.ttl</strong> parameter to trigger automatic cleanups.</p>
</li>
<li><p>Keep in mind that repartitioning your data is a fairly expensive operation. Spark also has an optimized version of repartition() called coalesce() that allows minimum the data movement, but only if you are decreasing the number of RDD partitions.</p>
</li>
<li><p>In a typical Spark node，40% of the memory is used for computing while 60% is used for storing the data.</p>
</li>
<li><p><strong>cache()/persist()</strong> is also lays operation. However, unpersist() is non-lazy operation.</p>
</li>
</ol>
<h3 id="2-_Spark_SQL">2. Spark SQL</h3><ol>
<li><p>When working with a HiveContext, DataFrames can also be saved as persistent tables using the saveAsTable command. Unlike the registerTempTable command, saveAsTable will materialize the contents of the dataframe and create a pointer to the data in the HiveMetastore. Persistent tables will still exist even after your Spark program has restarted, as long as you maintain your connection to the same metastore. A DataFrame for a persistent table can be created by calling the table method on a SQLContext with the name of the table.</p>
</li>
<li><p>By default saveAsTable will create a “managed table”, meaning that the location of the data will be controlled by the metastore. Managed tables will also have their data deleted automatically when a table is dropped. However, Spark can also create temp table from data frame using rdd.registerTempTable(“table_name”). These tables are out of control by hive.</p>
</li>
<li><p>Spark SQL also supports reading and writing data stored in Apache Hive. However, since Hive has a large number of dependencies, it is not included in the default Spark assembly. Hive support is enabled by adding the -Phive and -Phive-thriftserver flags to Spark’s build. This command builds a new assembly jar that includes Hive. Note that this Hive assembly jar must also be present on all of the worker nodes, as they will need access to the Hive serialization and deserialization libraries (SerDes) in order to access data stored in Hive.</p>
</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkmemo.jpg" alt=""><br>Here, I am collecting the memo while I am learning the spark so that people like me can benef]]>
    </summary>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="spark" scheme="http://www.sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Tableau Your Big Data]]></title>
    <link href="http://www.sparkera.ca/2015/12/11/Tableau-Your-Big-Data/"/>
    <id>http://www.sparkera.ca/2015/12/11/Tableau-Your-Big-Data/</id>
    <published>2015-12-12T02:04:58.000Z</published>
    <updated>2015-12-19T01:06:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-02tableau" title="Ten tools for ten big data areas 02_Tableau" target="_blank">Ten tools for ten big data areas 02_Tableau</a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the second topic I have covered for series of talks about the <a href="http://sparkera.ca/tags/10t10a/" target="_blank" rel="external">Ten Tools for Ten Big Data Areas</a>. <a href="http://www.tableau.com" target="_blank" rel="external">Tableau</a> is one of few commercial soft that I have to recommend in the visualization area for big data.</p>
<p>The term visualization becomes very popular when tableau software comes into the picture. As I remember, It was four or five years ago. When we first time to see how business intelligence can be like this and how data visualization looks like. Tableau software comes up the concept of self BI or BI without developers. This really scares me as I am a developer. But later, I have said it is correct. With the help of tableau, people can more focus on the meaning of data instead the old ways we deal with reporting. In addition, it is amazing fast speed and user experience makes it top notch in the data visualization domain and business intelligence. In recent years, we can see lots of traditional BI company start doing what was tableau doing, but it is clear it is far more away in terms of performance and sense of experience.</p>
<p><img src="/images/tableauyourdata.jpg" alt=""><br>Tableau is very first few of tools which provide connectors for big data from Hadoop Hive connector, impala, to Spark SQL. Tableau also announced new direct-connection capabilities with InfoSphere BigInsights from IBM, along with new beta connectors for Amazon Elastic MapReduce from Amazon Web Services Inc.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/yPkuirrxzGekWG" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://www.sparkera.ca/tags/10t10a/"/>
    
      <category term="tableau" scheme="http://www.sparkera.ca/tags/tableau/"/>
    
      <category term="visualization" scheme="http://www.sparkera.ca/tags/visualization/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Ten Tools for Ten Big Data Areas]]></title>
    <link href="http://www.sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/"/>
    <id>http://www.sparkera.ca/2015/11/28/Ten-Tools-for-Ten-Big-Data-Areas/</id>
    <published>2015-11-28T21:04:58.000Z</published>
    <updated>2015-12-21T01:25:25.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special magic and power. Anyone who can own one of these weapons could become a master or leader who is not undefeatable.<br><img src="/images/10weapons.png" alt=""></p>
<p>Nowadays, the big data ecosystem becomes bigger and bigger. There are thousands of players in the big data landscape actively paying right now by creating thousands of tools, framework, solution, etc. However, I believe the legend are always among few of them. I choose 10 of the greatest tools as my recommendation for people really want to learn or use big data (see headline picture). </p>
<p>I have started the journey to introduce each of them which are free or not, but I guarantee the best from my experience. Here is the tag <a href="http://sparkera.ca/tags/10t10a/" target="_blank" rel="external">Ten Tools for Ten Big Data Areas</a> for this series of presentations as well as direct link for each of them. Let’s look forward all of them in future…</p>
<ol>
<li><a href="http://sparkera.ca/2015/10/24/Informatica-in-Big-Data/" target="_blank" rel="external">Informatica in Big Data</a></li>
<li><a href="http://sparkera.ca/2015/12/11/Tableau-Your-Big-Data/" target="_blank" rel="external">Tableau Your Big Daya</a></li>
<li><a href="http://sparkera.ca/2015/12/19/Light-Big-Data-With-Spark/" target="_blank" rel="external">Light Big Data with Apache Spark</a></li>
<li><a href="http://sparkera.ca/2015/12/20/Build-Big-Data-Warehousing-With-Hive/" target="_blank" rel="external">Build Big Data Warehouse with Apache Hive</a></li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/10tools.png" alt=""></p>
<p>In the ancient of China, it is said there are ten legend weapons. Each of them has special ]]>
    </summary>
    
      <category term="10t10a" scheme="http://www.sparkera.ca/tags/10t10a/"/>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Informatica in Big Data]]></title>
    <link href="http://www.sparkera.ca/2015/10/24/Informatica-in-Big-Data/"/>
    <id>http://www.sparkera.ca/2015/10/24/Informatica-in-Big-Data/</id>
    <published>2015-10-25T03:04:58.000Z</published>
    <updated>2015-12-19T01:09:44.000Z</updated>
    <content type="html"><![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/WillDu1/ten-tools-for-ten-big-data-areas-01-informatica-54334595" title="Ten tools for ten big data areas 01 informatica " target="_blank">Ten tools for ten big data areas 01 informatica </a> </strong> from <strong><a href="//www.slideshare.net/WillDu1" target="_blank">Will Du</a></strong> </div></p>
<p>Above presentation is the first topic I’ll cover for series of talks about the <a href="http://sparkera.ca/tags/10t10a/" target="_blank" rel="external">Ten Tools for Ten Big Data Areas</a>. I came up this series of topics from ten artifacts from ancient China when people are honored to get one of the ten artifacts.</p>
<p>In terms of data integrations of big data, I pick up Informatica, which is used to be a public company in NASDAQ as INFA. In the middle of this year, Informatica announced the successful completion of its acquisition by a company controlled by the Permira funds and Canada Pension Plan Investment Board (CPPIB). Additionally Informatica announced that Microsoft Corporation and Salesforce Ventures have agreed to become strategic investors in the company alongside the Permira funds and CPPIB. The acquisition is valued at approximately $5.3 billion, with Informatica stockholders receiving $48.75 in cash per share.</p>
<p>Especially in big data area, Informatica has leading product called Informatica big data edition - developer. This is brand new tool in the Informatica family. It has new user interface based on Eclipse. It is single tool including development all ETL job components. In Informatica developer, this is no more session. For instead, there is no concept called application. We can add mapping or workflow in the application to deploy and run the bulk of ETL jobs as an application.</p>
<p>The main advantage of Informatica developer is that it converts the ETL logic/mapping into Hive query and execute it on top of Hadoop cluster. For example, you can even push a none-Hadoop related jobs running on top of Hadoop. This advantage not only make ETL job leverages the power of computing resource of Hadoop but also get rid of additional budget for a dedicated ETL server cluster like what’s in the Informatica PowerCenter period. In addition, this design has lots of potential when Hive evolves in the big data ecosystem, such as Hive over Spark. In future, Informatica developer will be able to leverage more distributed computing framework beyond of Hadoop, such as Spark for better performance.</p>
<p>However, there are still limitations for this tool which is still new in the Informatica family. Below are some limitations I come across recently.</p>
<ul>
<li>Does not support Hive table in complex formats, such as Avro, etc</li>
<li>Does not support write into buckets tables</li>
<li>Does not support using parameters in row level, complex data objects path.</li>
<li>Does not support to return target successful or failed rows from mapping</li>
<li>Cannot run the workflow or application straightforward in the developer tool</li>
<li>None of errors and exceptions are reported at run time when running in Hive mode</li>
<li>Overall reliability need to be improved, such as OOM, exception on data adapters</li>
</ul>
<p>Alternatives, there are also other ETL tools for choice. <a href="https://www.talend.com/" target="_blank" rel="external">Talend</a> and <a href="http://www.pentaho.com/" target="_blank" rel="external">Pentaho</a> all provide big data version of tools. But, their big data version almost the same to the regular one except having common big data tool connector shipped. Therefore, these tools can read or write data between Hadoop rather than running on top of Hadoop.</p>
<p>There is another new data flow tool which people may pay attention, <a href="https://nifi.apache.org/" target="_blank" rel="external">Apache NiFi</a>, which supports powerful and scalable directed graphs of data routing, transformation, and system mediation logic. The company behind this tool is called Onyara, an early-stage startup acquired by Hontonworks in Auguest of 2015.<br><img src="/images/bde.jpg" alt=""></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><iframe src="//www.slideshare.net/slideshow/embed_code/key/LQnShEqBTdAqvj" width="595" height="485" frameborder="0" marginwidth="0" margi]]>
    </summary>
    
      <category term="10t10a" scheme="http://www.sparkera.ca/tags/10t10a/"/>
    
      <category term="informatica" scheme="http://www.sparkera.ca/tags/informatica/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Cloudera Launches One Platform Initatives To Advance Spark]]></title>
    <link href="http://www.sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/"/>
    <id>http://www.sparkera.ca/2015/09/23/Cloudera-Launches-One-Platform-Initatives-To-Advance-Spark/</id>
    <published>2015-09-23T15:27:57.000Z</published>
    <updated>2015-09-24T02:56:23.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that the next important initiatives for Couldera - One Platform to advance their investment on Apache Spark.</p>
<p>The Spark is originally invented by few guys who started up the <a href="https://databricks.com/" target="_blank" rel="external">Databrick</a>. Later, Spark catches most attention from big data communities and companies by its high-performance in-memory computing framework, which can run on top of Hadoop Yarn. As a result, more and more companies start switching their MapReduce jobs to Spark and few of them already have big cluster deployed in production. Few months earlier, IBM has claimed that they would have 5000+ developers working in Apache Spark to make it better (It is heard that the core DB2 development force are reassigned to this new mission inside the IBM, not sure.). Clearly, the One Platform initiative is an echo for IBM’s saying from Cloudera who always believe itself a leader in the domain in big data. Cloudera is not likely to leave IBM alone to take this delicious fruit - Spark. We are waiting for more actions from other company, such as Hortonworks, MapR. I do not believe they just keep silent. Or maybe there is the underlying discussion to acquire the Databrick, who knows.</p>
<p>The One Platform initiative has covers four areas of efforts including security, scale, management, streaming. For more information regarding the One Platform initiative, please refer to the <a href="http://vision.cloudera.com/one-platform/" target="_blank" rel="external">Cloudera post</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/onep.png" alt=""><br>In the early of this September, the Chief Strategy Offer of Cloudera Mike Olson has announced that]]>
    </summary>
    
      <category term="spark" scheme="http://www.sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Get Git Modified But Untracked Content Checked In]]></title>
    <link href="http://www.sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/"/>
    <id>http://www.sparkera.ca/2015/08/21/Get-Git-Modified-But-Untracked-Content-Checked-In/</id>
    <published>2015-08-21T15:27:57.000Z</published>
    <updated>2015-08-22T18:11:54.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo project folder. I also keep the source code in the github in case I lost the source code. However, when I run the <code>git add .</code> and <code>git status</code>. It shows below error messages saying the theme folder is not tracked content. Most time, I did not check the git status - bad habit. I only realize that I miss the theme files when I try to rebuild the Hexo site from home.<br><img src="/images/gituntracked.png" alt=""><br>After searching a while form Google, I got my issues resolved and share the steps below for reference.</p>
<ul>
<li>Removed the .git directories from the directories (In my case, ../theme/hueman/.git)</li>
<li>Ran git rm -rf –cached <the untracked="" directory=""> (In my case, /C/Users/ddu/Git/sparkera/themes/hueman)</the></li>
<li>Re-added the directories with a <code>git add .</code> and check by <code>git status</code>.</li>
</ul>
<p>Then all the untracked files are added. Then you can do <code>git commit -m</code> and <code>git push</code> the submit all the changes.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/gettingStartedGit.jpg" alt=""><br>Recently, I migrate this site to Hexo. I download the theme from github to the Hexo p]]>
    </summary>
    
      <category term="git" scheme="http://www.sparkera.ca/tags/git/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hadoop Streaming]]></title>
    <link href="http://www.sparkera.ca/2015/06/21/Hadoop%20Streaming/"/>
    <id>http://www.sparkera.ca/2015/06/21/Hadoop Streaming/</id>
    <published>2015-06-21T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic API which allows writing Mappers and Reduces in any language. </p>
<ul>
<li>Develop MapReduce jobs in practically any language</li>
<li>Uses Unix Streams as communication mechanism between Hadoop and your code</li>
<li>Any language that can read standard input and write are supported</li>
</ul>
<p>Few good use-cases:</p>
<ul>
<li>Text processing - scripting languages do well in text analysis</li>
<li>Utilities and/or expertise in languages other than Java</li>
</ul>
<h4 id="2-_Process_Flow">2. Process Flow</h4><p>Below is how streaming processing</p>
<ul>
<li>Map input passed over standard input</li>
<li>Map processes input line-by-line</li>
<li>Map writes output to standard output - Key-value separate by tab</li>
<li>Reduce input passed over standard input<ul>
<li>Same as mapper output – key-value pairs separated by tab</li>
<li>Input is sorted by key</li>
</ul>
</li>
<li>Reduce writes output to standard output</li>
</ul>
<p><img src="/images/hadoopstreaming.png" alt="&quot;avatar&quot;"></p>
<h4 id="3-_Example_of_mapper">3. Example of mapper</h4><p><strong>mapper.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python </span></span><br><span class="line">import sys </span><br><span class="line"><span class="comment"># mapper.py </span></span><br><span class="line"><span class="comment"># input comes from STDIN (standard input) </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>: </span><br><span class="line"><span class="comment"># remove leading and trailing white space </span></span><br><span class="line"><span class="built_in">line</span> = <span class="built_in">line</span>.strip() </span><br><span class="line"><span class="comment"># split the line into words </span></span><br><span class="line"><span class="keyword">words</span> = <span class="built_in">line</span>.<span class="built_in">split</span>() </span><br><span class="line"><span class="comment"># increase counters for word in words: </span></span><br><span class="line"><span class="comment"># write the results to STDOUT (standard output); </span></span><br><span class="line"><span class="comment"># what we output here will be the input for the </span></span><br><span class="line"><span class="comment"># Reduce step, i.e. the input for reducer.py </span></span><br><span class="line"><span class="comment"># tab-delimited; the trivial word count is 1 </span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">word</span> <span class="operator">in</span> <span class="keyword">words</span></span><br><span class="line">print <span class="string">'%s\t%s'</span> % (<span class="built_in">word</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="4-_Example_of_reducer">4. Example of reducer</h4><p><strong>reducer.py</strong>  </p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#reducer.py</span></span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">current_word = None</span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line"><span class="built_in">word</span> = None</span><br><span class="line"></span><br><span class="line"><span class="comment"># input comes from STDIN</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">line</span> <span class="operator">in</span> sys.<span class="keyword">stdin</span>:</span><br><span class="line">    <span class="comment"># remove leading and trailing white space</span></span><br><span class="line">    <span class="built_in">line</span> = <span class="built_in">line</span>.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parse the input we got from mapper.py</span></span><br><span class="line">    <span class="built_in">word</span>, count = <span class="built_in">line</span>.<span class="built_in">split</span>(<span class="string">'\t'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert count (currently a string) to int</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = int(count)</span><br><span class="line">    except ValueError:</span><br><span class="line">        <span class="comment"># count was not a number, so silently ignore/discard this line</span></span><br><span class="line">        continue</span><br><span class="line"></span><br><span class="line">    <span class="comment"># this IF-switch works because Hadoop sorts map output by key before passed to the reducer</span></span><br><span class="line">    <span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> current_word:</span><br><span class="line">            <span class="comment"># write result to STDOUT</span></span><br><span class="line">            print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = <span class="built_in">word</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># do not forget to output the last word if needed!</span></span><br><span class="line"><span class="keyword">if</span> current_word == <span class="built_in">word</span>:</span><br><span class="line">    print <span class="string">'%s\t%s'</span> % (current_word, current_count)</span><br></pre></td></tr></table></figure>
<h4 id="5-_Run_the_job">5. Run the job</h4><ul>
<li>Test in local mode from Linux pipe</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat testText.txt | mapper.py | sort | reducer.py</span><br><span class="line">a <span class="number">1</span></span><br><span class="line">h <span class="number">1</span></span><br><span class="line">i <span class="number">4</span></span><br><span class="line">s <span class="number">1</span></span><br><span class="line">t <span class="number">5</span></span><br><span class="line">v <span class="number">1</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Run in the cluster</li>
</ul>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hadoop/yarn jar <span class="variable">$HADOOP_HOME</span>/share/hadoop/tools/lib/hadoop-streaming-*<span class="class">.jar</span> \</span><br><span class="line">-D mapred<span class="class">.job</span><span class="class">.name</span>=<span class="string">"Count Job via Streaming"</span> \</span><br><span class="line">-files <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/mapper<span class="class">.py</span>, <span class="variable">$HADOOP_SAMPLES_SRC</span>/scripts/reducer<span class="class">.py</span> \</span><br><span class="line">-<span class="tag">input</span> /training/input/hamlet<span class="class">.txt</span> \</span><br><span class="line">-output /training/output/ \</span><br><span class="line">-mapper mapper<span class="class">.py</span> \</span><br><span class="line">-combiner reducer<span class="class">.py</span> \</span><br><span class="line">-reducer reducer.py</span><br></pre></td></tr></table></figure>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/hadoopstream.jpg" alt=""></p>
<h4 id="1-_Streaming_Overview">1. Streaming Overview</h4><p>Hadoop Streaming is a generic]]>
    </summary>
    
      <category term="hadoop" scheme="http://www.sparkera.ca/tags/hadoop/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Constructor vs. Java Constructor]]></title>
    <link href="http://www.sparkera.ca/2015/04/20/Scala%20and%20Java%20Constructors/"/>
    <id>http://www.sparkera.ca/2015/04/20/Scala and Java Constructors/</id>
    <published>2015-04-20T04:00:00.000Z</published>
    <updated>2015-09-23T23:44:52.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="2-_Constructor_With_Private_Attribute">2. Constructor With Private Attribute</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">final</span> Bar bar;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(<span class="keyword">private</span> <span class="variable"><span class="keyword">val</span> bar</span>:Bar)</span><br></pre></td></tr></table></figure>
<h4 id="3-_Call_Super_Constructor">3. Call <em>Super</em> Constructor</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Foo</span>() <span class="keyword">extends</span> <span class="title">SuperFoo</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">public</span> Foo(Bar bar) &#123;   </span><br><span class="line">      <span class="keyword">super</span>(bar);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(<span class="title">bar</span>:<span class="title">Bar</span>) <span class="keyword">extends</span> <span class="title">SuperFoo</span>(<span class="title">bar</span>) </span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-_Multiple_Constructors">4. Multiple Constructors</h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cs"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span> &#123;  </span><br><span class="line">    <span class="keyword">public</span> Bar bar;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params"></span>) </span>&#123;   </span><br><span class="line">       <span class="keyword">this</span>(<span class="keyword">new</span> Bar());   </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Foo</span>(<span class="params">Bar bar</span>) </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>. bar = bar;   </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Scala Code</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Foo</span>(</span><span class="function"><span class="keyword">val</span> <span class="title">bar</span>:</span><span class="type">Bar</span>)&#123;  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>) = <span class="keyword">this</span>(<span class="keyword">new</span> <span class="type">Bar</span>)  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="5-_Methods_of_getter_and_setter">5. Methods of <em>getter</em> and <em>setter</em></h4><p><strong>Java Code</strong>  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">Foo</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">private</span> Bar bar;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">Foo</span><span class="params">(Bar bar)</span> </span>&#123;  </span><br><span class="line">       <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125; </span><br><span class="line">   <span class="function"><span class="keyword">public</span> Bar <span class="title">getBar</span><span class="params">()</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">return</span> bar;   </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setBar</span><span class="params">(Bar bar)</span> </span>&#123;   </span><br><span class="line">      <span class="keyword">this</span>.bar = bar;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>1. Scala Code</strong></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="import"><span class="keyword">import</span> scala.reflect._  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Foo</span><span class="container">(@<span class="type">BeanProperty</span> <span class="title">var</span> <span class="title">bar</span>:<span class="type">Bar</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>2. Scala Code</strong></p>
<pre><code><span class="keyword">import</span> scala.reflect._  
<span class="class"><span class="keyword">class</span> <span class="title">Foo</span></span>(aBar:Bar) {  
    @BeanProperty  
    <span class="keyword">private</span> <span class="variable"><span class="keyword">var</span> bar</span> = aBar  
}
</code></pre>]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/scala-vs-java.png" alt=""></p>
<h4 id="1-_Constructor_With_Parameters">1. Constructor With Parameters</h4><p><strong>Ja]]>
    </summary>
    
      <category term="scala" scheme="http://www.sparkera.ca/tags/scala/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Hive Essentials Published]]></title>
    <link href="http://www.sparkera.ca/2015/03/15/Apache%20Hive%20Essentials%20Published/"/>
    <id>http://www.sparkera.ca/2015/03/15/Apache Hive Essentials Published/</id>
    <published>2015-03-15T04:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<li>My very first book</li>
<li>Also the first book on Apache Hive 1.0.0 in the world</li>
</ul>
<p>Check it out <a href="http://bit.ly/1LRkd5m" target="_blank" rel="external">here</a></p>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/apache-hive-essentials" target="_blank"><img src="/images/hivebooks.jpg" width="150" height="200" align="left"></a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Finally, I made it. I got it published after working for 6 monthes.</p>
<h3 id="Apache_Hive_Essentials">Apache Hive Essentials</h3><ul>
<]]>
    </summary>
    
      <category term="hadoop" scheme="http://www.sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://www.sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Hive and Hadoop Exceptions]]></title>
    <link href="http://www.sparkera.ca/2015/02/12/Hive%20and%20Hadoop%20Exceptions/"/>
    <id>http://www.sparkera.ca/2015/02/12/Hive and Hadoop Exceptions/</id>
    <published>2015-02-12T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/exceptions.png" alt=""></p>
<p>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports following exceptions</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org<span class="class">.apache</span><span class="class">.hadoop</span><span class="class">.hive</span><span class="class">.ql</span><span class="class">.metadata</span><span class="class">.HiveException</span>:java<span class="class">.io</span><span class="class">.IOException</span>:Filesystem closed</span><br></pre></td></tr></table></figure>
<p>According to the search <a href="http://mail-archives.apache.org/mod_mbox/hadoop-common-user/201207.mbox/%3CCAL=yAAE1mM-JRb=eJGkAtxWQ7AJ3e7WJCT9BhgWq7XDTNxrwfw@mail.gmail.com%3E" target="_blank" rel="external">here</a>, the mainly reason for this is that when multiple nodes read HFDS files if one node is offline, it will throw such exception when the other nodes are still reading the data cached. There are two ways to resolve this.</p>
<ul>
<li>Turn off JVM reuse</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>mapred.job.reuse.jvm.num.tasks<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>Disable caches</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">name</span>&gt;</span>fs.hdfs.impl.disable.cache<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">value</span>&gt;</span>true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/exceptions.png" alt=""></p>
<p>I installed Hive 1.0.0 on Hadoop 1.2.1. When I try to enter the Hive CLI, it reports fol]]>
    </summary>
    
      <category term="hadoop" scheme="http://www.sparkera.ca/tags/hadoop/"/>
    
      <category term="hive" scheme="http://www.sparkera.ca/tags/hive/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Data Analysis]]></title>
    <link href="http://www.sparkera.ca/2015/02/06/Data-Analysis/"/>
    <id>http://www.sparkera.ca/2015/02/06/Data-Analysis/</id>
    <published>2015-02-06T05:00:00.000Z</published>
    <updated>2015-09-23T23:45:21.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/Data-Analysis.jpg" alt=""></p>
<p>A friend of mine asked me what is data analysis. This is a simple but difficult question. It is simple because we talk about data analysis all the time and everywhere. It is difficult because there are so many ways of explaining it at different time. In the ear of big data, I think data analysis have three following areas.</p>
<ul>
<li><p>Flatten Analysis: Analysis is performed on the static data set from single dimentional view. Most analysis are simple enough focusing on the fixed scope of data. It is more or less like statistics, such as total, average, standard devidations, etc. Simple SQL or Excel tools can be used to do flatten data analysis.</p>
</li>
<li><p>Dimentional Analysis</p>
</li>
<li><p>Iterational Analysis</p>
</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/Data-Analysis.jpg" alt=""></p>
<p>A friend of mine asked me what is data analysis. This is a simple but difficult quest]]>
    </summary>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Data Lake Stages]]></title>
    <link href="http://www.sparkera.ca/2015/02/02/Data-Lake-Stages/"/>
    <id>http://www.sparkera.ca/2015/02/02/Data-Lake-Stages/</id>
    <published>2015-02-02T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/datalake.png" align="left"> <br><br><br>Edd has post a very impressive <a href="http://www.forbes.com/sites/edddumbill/2014/01/14/the-data-lake-dream/" target="_blank" rel="external">blog</a> about how Hadoop ecosystem influence the data lake in enterprise recently. It discussed about the four following stages when enterprise’s data evolution  to the dream of data lake. I also share some of mine as addition.</p>
<h5 id="Stage_1_-_Life_Before_Hadoop">Stage 1 - Life Before Hadoop</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage1.png" alt="Life Before Hadoop" title="Life Before Hadoop"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Applications stand alone with their databases</li>
<li>Some applications contribute data to a data warehouse</li>
<li>Analysts run reporting and analytics in data warehouse</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical enterprise data warehouse (EDW) happens. Most of data sets are structured and well-organized. </li>
</ul>
<h5 id="Stage_2_-_Hadoop_Is_Introduced">Stage 2 - Hadoop Is Introduced</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage2.png" alt="Hadoop Is Introduced" title="Hadoop Is Introduced"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Applications contribute data to Hadoop</li>
<li>Hadoop runs batch MapReduce jobs</li>
<li>Hadoop used for ETL into warehouse or analytic databases</li>
<li>Hadoop data reintroduced into applications</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical EDW and hadoop merge happens. Most of data sets are semi-structured and unstructured with structured data. </li>
<li>The hot data is more likely injected to EDW since EDW has better support to BI tools and faster response for ad-hoc query.</li>
<li>Data in Hadoop becomes a center data depository considering the data volume and management cost. Therefore, the EDW also injects its data to the Hadoop. Here is where I disagree with the Edd since I believe it is more like a data exchange (bidirections) instead of single direction in the picture.</li>
<li>Analytics over Hadoop/Big data starts as data verification, long period statistics, and trending calculations.</li>
</ul>
<h5 id="Stage_3_-_Growing_The_Data_Lake">Stage 3 - Growing The Data Lake</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage3.png" alt="Growing The Data Lake" title="Growing The Data Lake"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>Newly built systems center around Hadoop by default</li>
<li>Applications use each other’s data via Hadoop</li>
<li>Interactive use of Hadoop as in-Hadoop databases deployed (e.g. Impala, Greenplum, Spark)</li>
<li>Hadoop becomes a default data destination, governance and metadata become important</li>
<li>Data warehouse use becomes the exception, where legacy or special requirements dictate</li>
<li>External data sources integrated via Hadoop</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical EDW being replaced happens. </li>
<li>Analytics over Hadoop becomes well accepted in terms of performance and compatibility.</li>
<li>However, the Hadoop is still playing role of OLAP instead of OLTP</li>
</ul>
<h5 id="Stage_4_-_Data_Lake_And_Application_Cloud">Stage 4 - Data Lake And Application Cloud</h5><p><img src="http://b-i.forbesimg.com/edddumbill/files/2014/01/stage4.png" alt="Data Lake And Application Cloud" title="Data Lake And Application Cloud"></p>
<p>In this stage, the enterprise data architecture has following characteristics.</p>
<ul>
<li>New applications are built on a Hadoop application platform around the data lake</li>
<li>Hadoop matures as an elastic distributed data computing platform, for both operational and analytical functions</li>
<li>Data lake adds security and governance layers</li>
<li>Data availability increases, application deployment time decreases</li>
<li>Some apps still have special or legacy needs and execute independently</li>
</ul>
<p>What’s more:</p>
<ul>
<li>This is a period where typical Data Oriented Architecture (DOA) starts.</li>
<li>Hadoop becomes the central, operational, analytical of enterprise data lake.</li>
<li>Different data lakes can also flow/exchange with each other by data services in terms of <strong><em>DATA OCEAN</em></strong></li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/datalake.png" align="left"> <br><br><br>Edd has post a very impressive <a href="http://www.forbes.com/sites/edddumbill/]]>
    </summary>
    
      <category term="bigdata" scheme="http://www.sparkera.ca/tags/bigdata/"/>
    
      <category term="hadoop" scheme="http://www.sparkera.ca/tags/hadoop/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Setup Spark in MAC]]></title>
    <link href="http://www.sparkera.ca/2015/01/23/Setup-Spark-In-MAC/"/>
    <id>http://www.sparkera.ca/2015/01/23/Setup-Spark-In-MAC/</id>
    <published>2015-01-23T05:00:00.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/sparkmac.jpg" alt=""><br>It is great to see that Brew supports install Spark. It makes installation of Spark quite easier in Mac. I just follow few steps to get my spark instance installed locally.</p>
<h4 id="1-_Install_brew_utility-">1. Install brew utility.</h4><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">mymac:</span>$ ruby -e <span class="string">"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</span></span><br><span class="line">==&gt; This script will <span class="string">install:</span></span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/bin/</span>brew</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/Library/</span>...</span><br><span class="line"><span class="regexp">/usr/</span>local<span class="regexp">/share/</span>man<span class="regexp">/man1/</span>brew<span class="number">.1</span></span><br><span class="line">==&gt; The following directories will be made group <span class="string">writable:</span></span><br><span class="line"><span class="regexp">/usr/</span>local/.</span><br><span class="line"><span class="regexp">/usr/</span>local/bin</span><br><span class="line">==&gt; The following directories will have their group set to <span class="string">admin:</span></span><br><span class="line"><span class="regexp">/usr/</span>local/.</span><br><span class="line"><span class="regexp">/usr/</span>local/bin</span><br><span class="line"></span><br><span class="line">Press RETURN to <span class="keyword">continue</span> or any other key to abort</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/chmod g+rwx /</span>usr<span class="regexp">/local/</span>. <span class="regexp">/usr/</span>local/bin</span><br><span class="line"><span class="string">Password:</span></span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>usr<span class="regexp">/bin/</span>chgrp admin <span class="regexp">/usr/</span>local<span class="regexp">/. /</span>usr<span class="regexp">/local/</span>bin</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/mkdir /</span>Library<span class="regexp">/Caches/</span>Homebrew</span><br><span class="line">==&gt; <span class="regexp">/usr/</span>bin<span class="regexp">/sudo /</span>bin<span class="regexp">/chmod g+rwx /</span>Library<span class="regexp">/Caches/</span>Homebrew</span><br><span class="line">==&gt; Downloading and installing Homebrew...</span><br><span class="line"><span class="string">remote:</span> Counting <span class="string">objects:</span> <span class="number">226972</span>, done.</span><br><span class="line"><span class="string">remote:</span> Compressing <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">59619</span>/<span class="number">59619</span>), done.</span><br><span class="line"><span class="string">remote:</span> Total <span class="number">226972</span> (delta <span class="number">166103</span>), reused <span class="number">226972</span> (delta <span class="number">166103</span>)</span><br><span class="line">Receiving <span class="string">objects:</span> <span class="number">100</span>% (<span class="number">226972</span><span class="regexp">/226972), 52.13 MiB | 1021.00 KiB/</span>s, done.</span><br><span class="line">Resolving <span class="string">deltas:</span> <span class="number">100</span>% (<span class="number">166103</span>/<span class="number">166103</span>), done.</span><br><span class="line">From <span class="string">https:</span><span class="comment">//github.com/Homebrew/homebrew</span></span><br><span class="line"> * [<span class="keyword">new</span> branch]      master     -&gt; origin/master</span><br><span class="line">HEAD is now at e58a69c <span class="string">points2grid:</span> update <span class="number">1.3</span><span class="number">.0</span> bottle.</span><br><span class="line">==&gt; Installation successful!</span><br><span class="line">==&gt; Next steps</span><br><span class="line">Run `brew doctor` before you install anything</span><br><span class="line">Run `brew help` to get started</span><br></pre></td></tr></table></figure>
<h4 id="2-_Install_the_spark_from_Brew">2. Install the spark from Brew</h4><figure class="highlight coffeescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mymac</span>:$ brew install apache-spark</span><br><span class="line">=<span class="function">=&gt;</span> Downloading </span><br><span class="line"><span class="attribute">http</span>:<span class="regexp">//</span>d3kbcqa49mib13.cloudfront.net/spark-<span class="number">1.2</span><span class="number">.0</span>-bin-hadoop2<span class="number">.4</span>.t</span><br><span class="line"><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span><span class="comment">######</span> <span class="number">100.0</span>% /usr/local/Cellar/apache-spark/<span class="number">1.2</span><span class="number">.0</span>: <span class="number">283</span> files, <span class="number">234</span>M, built <span class="keyword">in</span> <span class="number">24.8</span> minutes</span><br></pre></td></tr></table></figure>
<h4 id="3-_Start_the_spark_shell-">3. Start the spark shell.</h4><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">bash-<span class="number">3.2</span>$ spark-shell</span><br><span class="line">Spark <span class="keyword">assembly</span> <span class="keyword">has</span> been built <span class="keyword">with</span> Hive, including Datanucleus jars <span class="keyword">on</span> classpath</span><br><span class="line"><span class="keyword">Using</span> Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: Changing view acls to: dayongd</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: Changing modify acls to: dayongd</span><br><span class="line">15/01/23 20:21:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(dayongd); users with modify permissions: Set(dayongd)</span><br><span class="line">15/01/23 20:21:39 INFO HttpServer: Starting HTTP Server</span><br><span class="line">15/01/23 20:21:39 INFO Utils: Successfully started service '</span>HTTP <span class="keyword">class</span> server<span class="string">' on port 56839.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  '</span>_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version <span class="number">1.2</span>.<span class="number">0</span></span><br><span class="line">      /_/</span><br><span class="line"> </span><br><span class="line"><span class="keyword">Using</span> Scala version <span class="number">2.10</span>.<span class="number">4</span> (Java HotSpot(TM) <span class="number">64</span>-Bit Server VM, Java <span class="number">1.7</span>.<span class="number">0</span>_67)</span><br><span class="line"><span class="keyword">Type</span> <span class="keyword">in</span> expressions <span class="keyword">to</span> have them evaluated.</span><br><span class="line"><span class="keyword">Type</span> :help <span class="keyword">for</span> more information.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> WARN Utils: Your hostname, mymac resolves <span class="keyword">to</span> a loopback address: <span class="number">127.0</span>.<span class="number">0.1</span>; <span class="keyword">using</span> <span class="number">192.168</span>.<span class="number">3.7</span> instead (<span class="keyword">on</span> <span class="keyword">interface</span> en1)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> WARN Utils: <span class="keyword">Set</span> SPARK_LOCAL_IP <span class="keyword">if</span> you need <span class="keyword">to</span> bind <span class="keyword">to</span> another address</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: Changing view acls <span class="keyword">to</span>: dayongd</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: Changing modify acls <span class="keyword">to</span>: dayongd</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users <span class="keyword">with</span> view permissions: <span class="keyword">Set</span>(dayongd); users <span class="keyword">with</span> modify permissions: <span class="keyword">Set</span>(dayongd)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO Slf4jLogger: Slf4jLogger started</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">44</span> INFO Remoting: Starting remoting</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO Remoting: Remoting started; listening <span class="keyword">on</span> addresses :[akka.tcp:<span class="comment">//sparkDriver@192.168.3.7:56841]</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO Utils: Successfully started service <span class="string">'sparkDriver'</span> <span class="keyword">on</span> port <span class="number">56841</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO SparkEnv: Registering MapOutputTracker</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO SparkEnv: Registering BlockManagerMaster</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO DiskBlockManager: Created local directory at /<span class="keyword">var</span>/folders/zp/ns8pgfr91yj93hglw1mncph9ytq52s/T/spark-local-<span class="number">201501232021450</span>b71</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> INFO MemoryStore: MemoryStore started <span class="keyword">with</span> capacity <span class="number">265.4</span> MB</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">45</span> WARN NativeCodeLoader: Unable <span class="keyword">to</span> load native-hadoop <span class="keyword">library</span> <span class="keyword">for</span> your <span class="keyword">platform</span>... <span class="keyword">using</span> builtin-java classes <span class="keyword">where</span> applicable</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO HttpFileServer: HTTP File server directory <span class="keyword">is</span> /<span class="keyword">var</span>/folders/zp/ns8pgfr91yj93hglw1mncph9ytq52s/T/spark-e7b15fc0-<span class="number">7</span>c73-<span class="number">4</span>ddb-<span class="number">9</span>c71-ffa1e83f255a</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO HttpServer: Starting HTTP Server</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Utils: Successfully started service <span class="string">'HTTP file server'</span> <span class="keyword">on</span> port <span class="number">56842</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Utils: Successfully started service <span class="string">'SparkUI'</span> <span class="keyword">on</span> port <span class="number">4040</span>.</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO SparkUI: Started SparkUI at http:<span class="comment">//192.168.3.7:4040</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO Executor: <span class="keyword">Using</span> REPL <span class="keyword">class</span> URI: http:<span class="comment">//192.168.3.7:56839</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO AkkaUtils: Connecting <span class="keyword">to</span> HeartbeatReceiver: akka.tcp:<span class="comment">//sparkDriver@192.168.3.7:56841/user/HeartbeatReceiver</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO NettyBlockTransferService: Server created <span class="keyword">on</span> <span class="number">56843</span></span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMaster: Trying <span class="keyword">to</span> <span class="keyword">register</span> BlockManager</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMasterActor: Registering <span class="keyword">block</span> manager localhost:<span class="number">56843</span> <span class="keyword">with</span> <span class="number">265.4</span> MB RAM, BlockManagerId(&lt;driver&gt;, localhost, <span class="number">56843</span>)</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">46</span> INFO BlockManagerMaster: Registered BlockManager</span><br><span class="line"><span class="number">15</span>/<span class="number">01</span>/<span class="number">23</span> <span class="number">20</span>:<span class="number">21</span>:<span class="number">47</span> INFO SparkILoop: Created spark context..</span><br><span class="line">Spark context available <span class="keyword">as</span> sc.</span><br><span class="line">scala&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Note:">Note:</h3><p>If there is below exception, pls. make sure the local loop address is avaliable in the host file.  </p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.<span class="string">UnknownHostException:</span> <span class="string">mymac:</span> <span class="string">mymac:</span> nodename nor servname provided, or not known</span><br></pre></td></tr></table></figure>
<p>mymac$ sudo echo “127.0.0.1 mymac” &gt;&gt; /etc/hosts</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/sparkmac.jpg" alt=""><br>It is great to see that Brew supports install Spark. It makes installation of Spark quite easi]]>
    </summary>
    
      <category term="spark" scheme="http://www.sparkera.ca/tags/spark/"/>
    
      <category term="Blog" scheme="http://www.sparkera.ca/categories/Blog/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Scala Programming Books Reviews]]></title>
    <link href="http://www.sparkera.ca/2015/01/01/reviews-scala-programming/"/>
    <id>http://www.sparkera.ca/2015/01/01/reviews-scala-programming/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2015-09-23T23:46:47.000Z</updated>
    <content type="html"><![CDATA[<p><a href="https://onlybooks.org/beginning-scala-2nd-edition-32119" target="_blank"><img src="/images/beginscala.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Begining_Scala_2nd_ed">Begining Scala 2nd ed</h3><p><img src="/images/3.5star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>This book has very quick style of explaining scala with many detail examples. Even some examples has code defect and typo, it is easy to find out. The last part about scala best practice is really a valueable chapter. The other part is ok and cover everthing well enough but not deep enough.</p>
<hr>
<p><a href="http://shop.oreilly.com/product/0636920030287.do?sortby=publicationDate" target="_blank"><img src="http://akamaicovers.oreilly.com/images/0636920030287/lrg.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Learning_Scala_Practical_Functional_Programming_for_the_JVM">Learning Scala Practical Functional Programming for the JVM</h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>The book is my 1st book read about scala. It introduces the basic concet of scala as well as advanced topics. The last few chapters are little hard to read, but overall it is a great scala book having latest of scala features covered.</p>
<hr>
<p><a href="http://shop.oreilly.com/product/0636920026914.do" target="_blank"><img src="http://akamaicovers.oreilly.com/images/0636920026914/lrg.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Scala_Cookbook">Scala Cookbook</h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>The book is my 2nd book read about scala. This book is full of examples. It is more easier to learn thaning reading long chapters. I start reading it right now.</p>
<hr>
<p><a href="http://scalapuzzlers.com/" target="_blank"><img src="http://www.artima.com/images/puzzlersCover185x240.gif" width="150" height="200" align="right"></a></p>
<h3 id="Scala_Puzzlers">Scala Puzzlers</h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>The book is my 3rd book read about scala. It is a free book in github and it is a fun book about scala. I start reading it right now.</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="https://onlybooks.org/beginning-scala-2nd-edition-32119" target="_blank"><img src="/images/beginscala.jpg" width="150" height="2]]>
    </summary>
    
      <category term="books" scheme="http://www.sparkera.ca/tags/books/"/>
    
      <category term="scala" scheme="http://www.sparkera.ca/tags/scala/"/>
    
      <category term="Reviews" scheme="http://www.sparkera.ca/categories/Reviews/"/>
    
      <category term="Coding" scheme="http://www.sparkera.ca/categories/Reviews/Coding/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Apache Spark Books Reviews]]></title>
    <link href="http://www.sparkera.ca/2015/01/01/reviews-spark/"/>
    <id>http://www.sparkera.ca/2015/01/01/reviews-spark/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2015-12-18T02:03:47.000Z</updated>
    <content type="html"><![CDATA[<hr>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/spark-cookbook" target="_blank"><img src="/images/sparkcookbook.png" width="150" height="200" align="right"></a></p>
<h3 id="Apache_Spark_Cookbook">Apache Spark Cookbook</h3><p><img src="/images/4.5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent.   </li>
<li>Level Mid.   </li>
<li>Level Adv. </li>
</ul>
<p>I have completed the reading of this book. This is really a great a book of learning Apache Spark. It does not spend too many words on theory as well as the explanation. It introduces the code directly in front of you, simple and accurate. The way of learning from this book is very straightforward and efficient than other books. In addition, the book has a good focus on various areas of Apache Spark, especially on machine learning and graphic tops. Even it is a little bit rush, but it is ok as it is a cookbook. If you are a user who wants to start to learn Spark immediatelly and quickly, this is the book you should definatelly go for.</p>
<hr>
<p><a href="http://shop.oreilly.com/product/0636920028512.do" target="_blank"><img src="/images/learningspark.png" width="150" height="200" align="right"></a></p>
<h3 id="Learning_Spark_-_Lightning-Fast_Big_Data_Analysis">Learning Spark - Lightning-Fast Big Data Analysis</h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent.   </li>
<li>Level Mid.   </li>
<li>Level Adv. </li>
</ul>
<p>Start reading it.</p>
]]></content>
    <summary type="html">
    <![CDATA[<hr>
<p><a href="https://www.packtpub.com/big-data-and-business-intelligence/spark-cookbook" target="_blank"><img src="/images/sparkcookbook]]>
    </summary>
    
      <category term="books" scheme="http://www.sparkera.ca/tags/books/"/>
    
      <category term="spark" scheme="http://www.sparkera.ca/tags/spark/"/>
    
      <category term="Reviews" scheme="http://www.sparkera.ca/categories/Reviews/"/>
    
      <category term="Big Data" scheme="http://www.sparkera.ca/categories/Reviews/Big-Data/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Data Science and Data Mining Books Reviews]]></title>
    <link href="http://www.sparkera.ca/2015/01/01/reviews-data-science/"/>
    <id>http://www.sparkera.ca/2015/01/01/reviews-data-science/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/Data-Scientist.jpg" alt=""><br><a href="http://shop.oreilly.com/product/0636920025054.do?sortby=bestSellers" target="_blank"><img src="/images/AgileDataScience.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Agile_Data_Science_-_Building_Data_Analytics_Applications_with_Hadoop">Agile Data Science - Building Data Analytics Applications with Hadoop</h3><p><img src="/images/2.5star.png" width="150" height="50" align="left"><br></p>
<ul>
<li>Level Mid.   </li>
<li>Level Adv. </li>
</ul>
<p>The book looks like a case study from beginning to the end. It has lots of code covering implementation of one big data project. It requires coding skills of Python.</p>
<hr>
<p><a href="http://shop.oreilly.com/product/0636920028529.do" target="_blank"><img src="/images/doingdatascience.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Doing_Data_Science_-_Start_Talking_From_Frontline">Doing Data Science - Start Talking From Frontline</h3><p><img src="/images/4.5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent.   </li>
<li>Level Mid.   </li>
<li>Level Adv. </li>
</ul>
<p>Prepare to read it</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/Data-Scientist.jpg" alt=""><br><a href="http://shop.oreilly.com/product/0636920025054.do?sortby=bestSellers" target="_b]]>
    </summary>
    
      <category term="books" scheme="http://www.sparkera.ca/tags/books/"/>
    
      <category term="data science" scheme="http://www.sparkera.ca/tags/data-science/"/>
    
      <category term="Reviews" scheme="http://www.sparkera.ca/categories/Reviews/"/>
    
      <category term="Big Data" scheme="http://www.sparkera.ca/categories/Reviews/Big-Data/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Build Tools and Shell Program Books Reviews]]></title>
    <link href="http://www.sparkera.ca/2015/01/01/reviews-build-and-shell/"/>
    <id>http://www.sparkera.ca/2015/01/01/reviews-build-and-shell/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><a href="http://rockablepress.com/books/getting-good-with-git" target="_blank"><img src="/images/gettinggoodwithgit.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Geting_Good_with_GIT">Geting Good with GIT</h3><p><img src="/images/3.5star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
</ul>
<p>This is a tiny book for Git. It is good because it is short and simple. It covers basic usage of Git and a little about GitHub. It is particular good for people who want to learn Git from Zero. The is another link about GitHub call <a href="http://www.worldhello.net/gotgithub/" target="_blank" rel="external">GotGitHub</a>, which is also recommended reading after this. The only pity is that it does not cover any advanced topics. Here is my <a href="https://www.evernote.com/shard/s36/sh/f2cf70fa-6f1c-483b-9eab-23e52462f09e/4a50f05a3852edeb5a5fca7d03ed8d87" target="_blank" rel="external">my book note</a></p>
<hr>
<p><a href="http://www.packtpub.com/git-version-control-for-everyone/book" target="_blank"><img src="/images/gitversioncontrolforeveryone.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Git:_Version_Control_for_Everyone">Git: Version Control for Everyone</h3><p><img src="/images/3.5star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
</ul>
<p>This is a complete beginner’s workflow for version control of common documents and content. It has examples used are from non-techie, day to day computing activities for Git enter and middle level. It covers basic usage of Git and a little about Bitbucket in chapter 4. This book has very dtail of git command in terms of command output and how to run it in both git GUI and CLI.</p>
<hr>
<p><a href="http://www.amazon.cn/gp/product/B004CLZ7BA" target="_blank"><img src="/images/maven3inaction.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Maven_3_In_Action">Maven 3 In Action</h3><p><img src="/images/4.5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>This is a few books having details about Maven 3, which is am excellent Java project management tools. This book not only cover the basic knowledge of maven, but also introduce related tools, like Hudson. In addition, there are more good maven readings such as, <a href="http://books.sonatype.com/mvnref-book/reference/index.html" target="_blank" rel="external">Maven Complete Reference</a>, and <a href="http://www.tutorialspoint.com/maven/" target="_blank" rel="external">Maven Tutorial</a></p>
<hr>
<p><a href="http://linuxcommand.org/tlcl.php" target="_blank"><img src="/images/thelinuxcmdline.jpg" width="150" height="200" align="right"></a></p>
<h3 id="The_Linux_Command_Line">The Linux Command Line</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>This is a good begining guide for fresh people in linux programming. It is also a good quick review book. I never learn linux form books but from using experence and system manuals. This is a a free book which is recommanded to read to learn Linux in systematic way.</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="http://rockablepress.com/books/getting-good-with-git" target="_blank"><img src="/images/gettinggoodwithgit.jpg" width="150" heig]]>
    </summary>
    
      <category term="books" scheme="http://www.sparkera.ca/tags/books/"/>
    
      <category term="git" scheme="http://www.sparkera.ca/tags/git/"/>
    
      <category term="linux" scheme="http://www.sparkera.ca/tags/linux/"/>
    
      <category term="Reviews" scheme="http://www.sparkera.ca/categories/Reviews/"/>
    
      <category term="Coding" scheme="http://www.sparkera.ca/categories/Reviews/Coding/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Big Data Books Reviews]]></title>
    <link href="http://www.sparkera.ca/2015/01/01/reviews-big-data/"/>
    <id>http://www.sparkera.ca/2015/01/01/reviews-big-data/</id>
    <published>2015-01-01T13:00:11.000Z</published>
    <updated>2015-08-22T02:27:11.000Z</updated>
    <content type="html"><![CDATA[<p><img src="/images/bookreview3.png" alt=""></p>
<p><a href="http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449311520/ref=sr_1_1?ie=UTF8&qid=1368752048&sr=8-1&keywords=hadoop" target="_blank"><img src="/images/hadoopdefinitiveguide.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop:_The_Definitive_Guide">Hadoop: The Definitive Guide</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent.</li>
<li>Level Mid. </li>
<li>Level Adv. </li>
</ul>
<p>This is a really a good Hadoop book to recommend. I have read both 2nd and 3rd edition. The latest 3rd edition is based on the Hadoop 1.0. It covers almost everything on the Hadoop including Yarn. The author also has <a href="https://github.com/tomwhite/hadoop-book/" target="_blank"> github site </a> to share the code. Here is my <a href="https://www.evernote.com/shard/s36/sh/79d59799-3254-43a8-a316-e38e4760e3c8/a721c419a7cab4dd2592de6bc16203d4" target="_blank" rel="external">book note</a></p>
<hr>
<p><a href="http://www.amazon.com/Hadoop-Practice-Alex-Holmes/dp/1617290238/ref=sr_1_1?s=books&ie=UTF8&qid=1368753609&sr=1-1&keywords=hadoop+in+practice" target="_blank"><img src="/images/hadoopinpractice.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_In_Practice">Hadoop In Practice</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>This is pretty good book, especially in the data science chapter. The hive part is a little bit old than other latest Hadoop book. The reading experience is also good. I like the way it provides number of “TECHNIQUE”. It touches some new tool of big data that other books do not cover, such as Cloudera Crunch. There are no comments in the source code (request by publication), but there are enough comments added inline in the book.</p>
<hr>
<p><a href="http://www.amazon.com/Hadoop-Real-World-Solutions-Cookbook/dp/1849519129/ref=sr_1_1?s=books&ie=UTF8&qid=1366441101&sr=1-1" target="_blank"><img src="/images/hadooprealworld.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_Real_World_Solution_Cookbook">Hadoop Real World Solution Cookbook</h3><p><img src="/images/4.5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Mid. </li>
<li>Level Adv.</li>
</ul>
<p>The code has no comments with explanation below. The way is really not I like. If put the comments in code, the book may have less pages to read. In addition, there are logic mistakes in the book because copy &amp; paste error I think at least three – five time after I read 100s of pages, eg. p143 “hashset” should be “hashmap”. The charpter 7 starts looking good and deep which requires your knowledge on data mining and graph processing. This is a good tool reference book anyway</p>
<hr>
<p><a href="http://www.amazon.cn/gp/product/B009X25AI8/ref=s9_simh_gw_p14_d0_i2?pf_rd_m=A1AJ19PSB66TGU&pf_rd_s=center-2&pf_rd_r=1TNX5AHP1FEA00R1XW0B&pf_rd_t=101&pf_rd_p=58223152&pf_rd_i=899254051" target="_blank"><img src="/images/hadoopinactionchi.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_实战">Hadoop 实战</h3><p><img src="/images/3star.png" width="150" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid. </li>
</ul>
<p>This book is a Chinese book which has same name to below but with totally different. It covers majority Hadoop components and reading friendly. I only read the 1st edition, so the things are a little out of date. The 2ed is also on the shelf right now. Generally, it is just introduction and lacks of details and high skills.</p>
<hr>
<p><a href="http://www.manning.com/lam/" target="_blank"><img src="/images/hadoopinaction.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_In_Action">Hadoop In Action</h3><p><img src="/images/3star.png" width="150" height="50" align="left"><br></p>
<ul>
<li>Level Ent.</li>
<li>Level Mid. </li>
</ul>
<p>I got hard copy of this. This book is a little bit old based on Hadoop 0.19. It covers majority Hadoop components. It also has Chinese version.</p>
<hr>
<p><a href="http://packtlib.packtpub.com/hadoop-mapreduce-cookbook/book" target="_blank"><img src="/images/hadoopmrcookbook.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_MapReduce_Cookbook">Hadoop MapReduce Cookbook</h3><p><img src="/images/3star.png" width="150" height="50" align="left"><br></p>
<ul>
<li>Level Ent.</li>
<li>Level Mid. </li>
<li>Level Adv. </li>
</ul>
<p>Some sample Hadoop commands lack of necessary space between command/parameters. In Ch8, it provide some data analytics implementation using Java and MapReduce, which I did not see details like this in other books. It it worthy more time of reading this part.</p>
<hr>
<p><a href="http://www.amazon.com/MapReduce-Design-Patterns-Effective-Algorithms/dp/1449327176" target="_blank"><img src="/images/hadoopmrdp.jpg" width="150" height="200" align="right"></a></p>
<h3 id="MapReduce_Design_Patterns">MapReduce Design Patterns</h3><p><img src="/images/3star.png" width="150" height="50" align="left"><br></p>
<ul>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>The topic is really focus. The pattern is not that exciting comparing with Java’s in description. There is small values if you already read below other books. There are typos and mistakes. I cannot find the source code either.</p>
<hr>
<p><a href="http://ofps.oreilly.com/titles/9781449302641/" target="_blank"><img src="/images/programmingpig.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Programming_Pig">Programming Pig</h3><p><img src="/images/4star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Mid.  </li>
<li>Level Adv.  </li>
</ul>
<p>This is a tiny book about pig, around 200 pages. It covers everything. The extension of UDF parts lacks of enough examples. Also, these parts are a little bit hard for reading. I have also read the translation one, which is so so. You cannot find more examples of Pig than anywhere else. However, I expect there is another book I believe that could/should cover more practical examples and hands on scripts.</p>
<hr>
<p><a href="http://www.amazon.cn/Hadoop%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95-%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90MapReduce%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-%E8%91%A3%E8%A5%BF%E6%88%90/dp/B00CJ367IU/ref=sr_1_1?ie=UTF8&qid=1369267204&sr=8-1&keywords=hadoop%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95+%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90mapreduce%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86" target="_blank"><img src="/images/hadoopmapreduceinternals.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Hadoop_Mapreduce_Internals">Hadoop Mapreduce Internals</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Adv.   </li>
</ul>
<p>This book tells how map and reduce are implemented in source code level. It covers lots of detail that other book never mentioned. It can help reading the source code. This is kind of book helping uderstanding instead of practicing something. There are less code samples with book. The picture and comparing form in this book are really good for reading and undersanding.</p>
<hr>
<p><a href="http://www.packtpub.com/hbase-administration-for-optimum-database-performance-cookbook/book" target="_blank"><img src="/images/hbasecookbook.jpg" width="150" height="200" align="right"></a></p>
<h3 id="HBase_Administration_Cookbook">HBase Administration Cookbook</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Mid. </li>
<li>Level Adv. </li>
</ul>
<p>This book is for HBase administrators, developers, and will even help Hadoop administrators. You are not required to have HBase experience, but are expected to have a basic understanding of Hadoop and MapReduce. This is very practical tookit book for HBase admin. It does not talk more about API and focus on administration only.</p>
<hr>
<p><a href="http://shop.oreilly.com/product/0636920014348.do?sortby=publicationDate" target="_blank"><img src="/images/hbasedefinitiveguide.jpg" width="150" height="200" align="right"></a></p>
<h3 id="HBase:_The_Definitive_Guide">HBase: The Definitive Guide</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent.</li>
<li>Level Mid. </li>
<li>Level Adv. </li>
</ul>
<p>This is a really a good HBase book to recommend. This is the 1st edition and shows you how Apache HBase can fulfill your needs. As the open source implementation of Google’s BigTable architecture, HBase scales to billions of rows and millions of columns, while ensuring that write and read performance remain constant. The author also has <a href="https://github.com/larsgeorge/hbase-book" target="_blank"> github site </a> to share the code. I am still in reading for now and it is a little bit hard. </p>
<hr>
<p><a href="http://www.amazon.com/Big-Data-Revolution-Transform-Think/dp/0544002695/ref=sr_1_1?ie=UTF8&qid=1367466814&sr=8-1&keywords=Big+Date%3AA+Revolution+That+Will+Transform+How+We+Live%2C+Work%2C+and+Think" target="_blank"><img src="/images/bigdataera.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Big_Data:_A_Revolution_That_Will_Transform_How_We_Live,_Work,_and_Think">Big Data: A Revolution That Will Transform How We Live, Work, and Think</h3><p><img src="/images/5star.png" width="250" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
</ul>
<p>It is one of few books of big data using real example to tell what’s revolution brought by big data. The signatures of big data it describes are really impressive. This book motives readers to explore the value behind of big data. It is a good book to encourage people to explore the big data area.</p>
<hr>
<p><a href="http://www.packtpub.com/apache-hive-essentials-how-to/book" target="_blank"><img src="/images/instantapachehiveessentials.jpg" width="150" height="200" align="right"></a></p>
<h3 id="Instant_Apache_Hive_Essentials_How-to">Instant Apache Hive Essentials How-to</h3><p><img src="/images/3.5star.png" width="200" height="50" align="left"><br></p>
<ul>
<li>Level Ent. </li>
<li>Level Mid.   </li>
</ul>
<p>The book creates fast way to query data using hive in few hours. This is great than searching the apache confluence to see the breaked help documents especially for new hive users. The book has few pages to read and easier to understand. The author also gives level of complex for each chapters so that different level of users could quickly pick up what he/she needs.</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p><img src="/images/bookreview3.png" alt=""></p>
<p><a href="http://www.amazon.com/Hadoop-Definitive-Guide-Tom-White/dp/1449311520/ref=sr_1]]>
    </summary>
    
      <category term="books" scheme="http://www.sparkera.ca/tags/books/"/>
    
      <category term="Reviews" scheme="http://www.sparkera.ca/categories/Reviews/"/>
    
      <category term="Big Data" scheme="http://www.sparkera.ca/categories/Reviews/Big-Data/"/>
    
  </entry>
  
</feed>
